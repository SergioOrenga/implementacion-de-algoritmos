{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Práctica Final - Detección Trolls en Twitch_SergioDrive.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj8bcxFX85UO"
      },
      "source": [
        "# Práctica Final: detección de mensajes troll en chat de Twitch en tiempo real\n",
        "\n",
        "Durante este último año la plataforma de vídeo en streaming Twitch ha cogido mucha popularidad debido a la situación que hemos vivido debido al COVID-19. Por esto, mucha gente de todas las edades ha empezado a consumir esta plataforma de manera diaria.\n",
        "\n",
        "Como consecuencia, no sólo han aumentado las personas que ven contenido en Twitch, sino también el número de los denominados *trolls*, gente que pone comentarios ofensivos en los chat de los streamers.\n",
        "\n",
        "En esta práctica se desarrollará un sistema autónomo basado en IA y desplegado en GCP que detectará en tiempo real si los mensajes que se envían a un canal de Twitch son de un *troll* o no. La práctica constará de tres partes principales que serán evaluadas en la corrección:\n",
        "1. Entrenamiento e inferencia en Batch de un modelo usando Dataflow y AI Platform. **(3.5 puntos)**.\n",
        "2. Despliegue e inferencia online en microservicio con el modelo. **(3.5 puntos)**.\n",
        "3. Inferencia en streaming de un canal de Twitch con el microservicio anterior. **(3 puntos)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgE9XB8ACVmQ"
      },
      "source": [
        "# Configuración de nuestro proyecto en GCP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Los siguientes pasos es obligatorio realizarlos para seguir con la práctica.**\n",
        "\n",
        "1.   Selecciona o crea un proyecto en GCP\n",
        "2.   Asegurate de que la facturación está activada para tu proyecto.\n",
        "3.   [Habilita la API de Google Cloud Storage](https://console.cloud.google.com/apis/library/storage-component.googleapis.com?q=storage).\n",
        "4. [Habilita la API de Google Cloud Dataflow](https://console.cloud.google.com/apis/library/dataflow.googleapis.com?q=dataflow).\n",
        "5. [Habilita la API de Google AI Platform](https://console.cloud.google.com/apis/library/ml.googleapis.com?q=ai%20platform).\n",
        "6. Introduce tu ID de proyecto de GCP en la celda de abajo. Ejecuta la celda para asegurarnos de que el Cloud SDK usa el proyecto adecuado para todos los comandos en este notebook.\n",
        "\n",
        "**Nota**: Jupyter ejecuta las lineas con el prefijo `!` como comandos shell de consola, y puede usar variables de Python en los comandos añadiendoles el prefijo `$`."
      ],
      "metadata": {
        "id": "dNygS39SWgsL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAorVw7RCT9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c231c5e7-d4f8-4a12-f39f-79d9bf95fceb"
      },
      "source": [
        "PROJECT_ID = \"practica-desp-alg-sergio\" #@param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbAl8pSkCXCm"
      },
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this notebook locally, replace the string below with the\n",
        "# path to your service account key and run this cell to authenticate your GCP\n",
        "# account.\n",
        "else:\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS ''\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Zpx14FCfXy"
      },
      "source": [
        "BUCKET_NAME = \"practica-desp-alg-sergio-buck\" #@param {type:\"string\"}\n",
        "REGION = \"europe-west1\" #@param {type:\"string\"}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sólo si tu bucket aún no existe**: Ejecuta la siguiente celda para crear tu bucket en Cloud Storage."
      ],
      "metadata": {
        "id": "mo_6lYtMW2el"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil mb -l $REGION gs://$BUCKET_NAME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ35JZ-HXDeq",
        "outputId": "ef6a421e-c632-4951-c4fb-7b994f7cfa77"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://practica-desp-alg-sergio-buck/...\n",
            "ServiceException: 409 A Cloud Storage bucket named 'practica-desp-alg-sergio-buck' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, validamos que tenemos acceso al bucket de Cloud Storage mirando sus contenidos:"
      ],
      "metadata": {
        "id": "pICZ3jpMW6SN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE4V-Xt0ChSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957da48c-f9a5-4bf8-a2fd-11cc6633ad45"
      },
      "source": [
        "! gsutil ls -al gs://$BUCKET_NAME"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   2756023  2022-02-26T20:22:31Z  gs://practica-desp-alg-sergio-buck/data.json#1645906951286940  metageneration=1\n",
            "                                 gs://practica-desp-alg-sergio-buck/beam-temp/\n",
            "                                 gs://practica-desp-alg-sergio-buck/model/\n",
            "                                 gs://practica-desp-alg-sergio-buck/predictions/\n",
            "                                 gs://practica-desp-alg-sergio-buck/trainer/\n",
            "                                 gs://practica-desp-alg-sergio-buck/transformed_data/\n",
            "TOTAL: 1 objects, 2756023 bytes (2.63 MiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D8Qw14y_nRa"
      },
      "source": [
        "# Entrenamiento e inferencia en Batch\n",
        "\n",
        "Para esta primera parte se va a utilizar [Tweets Dataset for Detection of Cyber-Trolls](https://www.kaggle.com/dataturks/dataset-for-detection-of-cybertrolls). El objetivo es desarrollar un clasificador binario para detectar si el mensaje recibido es troll (1) o no (0). **Las métricas obtenidas del entrenamiento y la inferencia no se tendrán en cuenta para la evaluación de la práctica, la importancia está en la arquitectura de la solución**.\n",
        "\n",
        "A continuación os dejo un diagrama con la arquitectura que se va a desarrollar:\n",
        "\n",
        "![batch_diagram](https://drive.google.com/uc?export=view&id=1h1BkIunyKSkJYFRbXKNWpHOZ_rDUyGAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM_dh-471gIq"
      },
      "source": [
        "A continuación, se van a subir los datos de entrenamiento al bucket del proyecto que se haya creado. **Importante:** crea el bucket en una única región para evitar problemas más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqp4L_nmUjAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c0a056d-ec0b-4e79-a940-0521b85b2cc7"
      },
      "source": [
        "# Upload data to your bucket\n",
        "! wget https://storage.googleapis.com/twitch-practice-keepcoding/data.json -O - | gsutil cp - gs://$BUCKET_NAME/data.json"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-26 20:22:26--  https://storage.googleapis.com/twitch-practice-keepcoding/data.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.135.128, 74.125.142.128, 74.125.195.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.135.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2756023 (2.6M) [application/json]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                     0%[                    ]       0  --.-KB/s               Copying from <STDIN>...\n",
            "-                   100%[===================>]   2.63M  1.18MB/s    in 2.2s    \n",
            "\n",
            "2022-02-26 20:22:29 (1.18 MB/s) - written to stdout [2756023/2756023]\n",
            "\n",
            "/ [1 files][    0.0 B/    0.0 B]                                                \n",
            "Operation completed over 1 objects.                                              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCQQ9j2I11tg"
      },
      "source": [
        "Ahora se crea el directorio dónde vas a desarrollar esta primera parte de la práctica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsblBlJ6RrGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd441df-c485-4d00-c8ba-d07defd86fd5"
      },
      "source": [
        "%mkdir /content/batch"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/batch’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyK51quU1_oi"
      },
      "source": [
        "Se establece el directorio de trabajo que hemos creado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7ybSaotRwkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e70438f-939d-42c0-9c76-d5b1a4a91dba"
      },
      "source": [
        "import os\n",
        "\n",
        "# Set the working directory to the sample code directory\n",
        "%cd /content/batch\n",
        "\n",
        "WORK_DIR = os.getcwd()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/batch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NSd4bAo2aj_"
      },
      "source": [
        "Ahora se descargarán los datos en el workspace de Colab para trabajar en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOmUJSg1JCgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4229af5b-e51a-4376-84ff-e069b7ca1242"
      },
      "source": [
        "! wget https://storage.googleapis.com/twitch-practice-keepcoding/data.json"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-26 20:22:43--  https://storage.googleapis.com/twitch-practice-keepcoding/data.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 74.125.195.128, 74.125.197.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2756023 (2.6M) [application/json]\n",
            "Saving to: ‘data.json’\n",
            "\n",
            "\rdata.json             0%[                    ]       0  --.-KB/s               \rdata.json           100%[===================>]   2.63M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-02-26 20:22:43 (208 MB/s) - ‘data.json’ saved [2756023/2756023]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRoW6zyg2kEz"
      },
      "source": [
        "Se establecen las dependencias que se usarán en la práctica. Se pueden añadir y quitar las dependencias que no se usen o viceversa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s0-8_JK_z2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab568e15-ebbe-4a61-c93c-2b6a2b5ea37a"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "apache-beam[gcp]==2.24.0\n",
        "tensorflow\n",
        "gensim==3.6.0\n",
        "fsspec==0.8.4\n",
        "gcsfs==0.7.1\n",
        "numpy==1.20.0\n",
        "keras==2.8.0"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIl8lrPp2x2j"
      },
      "source": [
        "Instalamos las dependencias. **No olvidarse de reiniciar el entorno al instalar y establecer las variables y credenciales de GCP al arrancar.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS6P8A4c_8le",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70fa1be4-d16f-4f81-ba5e-ffacc94e9a24"
      },
      "source": [
        "! pip install -r requirements.txt"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache-beam[gcp]==2.24.0\n",
            "  Downloading apache_beam-2.24.0-cp37-cp37m-manylinux2010_x86_64.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (2.8.0)\n",
            "Requirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.6.0)\n",
            "Collecting fsspec==0.8.4\n",
            "  Downloading fsspec-0.8.4-py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 9.9 MB/s \n",
            "\u001b[?25hCollecting gcsfs==0.7.1\n",
            "  Downloading gcsfs-0.7.1-py2.py3-none-any.whl (20 kB)\n",
            "Collecting numpy==1.20.0\n",
            "  Downloading numpy-1.20.0-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 39.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras==2.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (2.8.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0->-r requirements.txt (line 4)) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (0.4.6)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (1.35.0)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2018.9)\n",
            "Collecting fastavro<0.24,>=0.21.4\n",
            "  Downloading fastavro-0.23.6-cp37-cp37m-manylinux2010_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 37.0 MB/s \n",
            "\u001b[?25hCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
            "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
            "Collecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "\u001b[K     |████████████████████████████████| 508 kB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.17.3)\n",
            "Collecting typing-extensions<3.8.0,>=3.7.0\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.43.0)\n",
            "Collecting mock<3.0.0,>=1.0.1\n",
            "  Downloading mock-2.0.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
            "Collecting pyarrow<0.18.0,>=0.15.1\n",
            "  Downloading pyarrow-0.17.1-cp37-cp37m-manylinux2014_x86_64.whl (63.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 63.8 MB 88 kB/s \n",
            "\u001b[?25hCollecting oauth2client<4,>=2.0.1\n",
            "  Downloading oauth2client-3.0.0.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.17.4)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Collecting google-cloud-spanner<2,>=1.13.0\n",
            "  Downloading google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255 kB)\n",
            "\u001b[K     |████████████████████████████████| 255 kB 54.5 MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1\n",
            "  Downloading google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[K     |████████████████████████████████| 267 kB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.0.3)\n",
            "Collecting google-cloud-vision<2,>=0.38.0\n",
            "  Downloading google_cloud_vision-1.0.0-py2.py3-none-any.whl (435 kB)\n",
            "\u001b[K     |████████████████████████████████| 435 kB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-datastore<2,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.8.0)\n",
            "Collecting google-cloud-pubsub<2,>=0.39.0\n",
            "  Downloading google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 56.1 MB/s \n",
            "\u001b[?25hCollecting grpcio-gcp<1,>=0.2.2\n",
            "  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting google-cloud-videointelligence<2,>=1.8.0\n",
            "  Downloading google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183 kB)\n",
            "\u001b[K     |████████████████████████████████| 183 kB 58.1 MB/s \n",
            "\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "\u001b[K     |████████████████████████████████| 173 kB 55.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<2,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.21.0)\n",
            "Collecting google-cloud-language<2,>=1.3.0\n",
            "  Downloading google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting cachetools<4,>=3.1.0\n",
            "  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting google-cloud-dlp<2,>=0.12.0\n",
            "  Downloading google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169 kB)\n",
            "\u001b[K     |████████████████████████████████| 169 kB 46.1 MB/s \n",
            "\u001b[?25hCollecting fasteners>=0.14\n",
            "  Downloading fasteners-0.17.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 6)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 6)) (4.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 6)) (57.4.0)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.4.1)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading grpc-google-iam-v1-0.12.3.tar.gz (13 kB)\n",
            "Collecting google-cloud-core<2,>=0.28.1\n",
            "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.26.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.54.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.6.2)\n",
            "Collecting pbr>=0.11\n",
            "  Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[K     |████████████████████████████████| 113 kB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.0.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 6)) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 6)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 6)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 6)) (2021.10.8)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (13.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.0.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 73.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.13.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.5.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.24.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 3)) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->-r requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 3)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 3)) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 3)) (3.3.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 3)) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 3)) (3.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 6)) (3.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 68.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 6)) (21.4.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 67.6 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Building wheels for collected packages: avro-python3, dill, future, google-apitools, grpc-google-iam-v1, oauth2client\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43513 sha256=cc321f2c353878ba1816d51d2d4d81afe9fba0fdf245f86838fc6518c034dc64\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=a149d4865b40f7a8f95d7cc5b60dc2101c0124db8fee26fba4b66e3c71750020\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=8ad9b5f2508d08bd786e5fe4a13479797480f315b2e593cc6d80240bf2d6cfde\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131039 sha256=eb672913dbb0b32d3307d2d603f4d1e08ea9951068a90eac4f17698182158f18\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/b5/2f/1cc3cf2b31e7a9cd1508731212526d9550271274d351c96f16\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-py3-none-any.whl size=18515 sha256=b37f54a9aa6c5164e4960c258ba8b65950a086861e24d7eebdf50c5cff077744\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/ee/67/2e444183030cb8d31ce8b34cee34a7afdbd3ba5959ea846380\n",
            "  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oauth2client: filename=oauth2client-3.0.0-py3-none-any.whl size=106375 sha256=2b874aa48bf27e25608b653adf7eb62e5b105b4e32c24996ef0765b344727339\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/73/7a/3b3f76a2142176605ff38fbca574327962c71e25a43197a4c1\n",
            "Successfully built avro-python3 dill future google-apitools grpc-google-iam-v1 oauth2client\n",
            "Installing collected packages: cachetools, typing-extensions, requests, pbr, numpy, multidict, grpcio-gcp, frozenlist, yarl, pymongo, pyarrow, oauth2client, mock, hdfs, grpc-google-iam-v1, google-cloud-core, future, fasteners, fastavro, dill, avro-python3, asynctest, async-timeout, aiosignal, tf-estimator-nightly, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-bigtable, google-apitools, fsspec, apache-beam, aiohttp, gcsfs\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 4.2.4\n",
            "    Uninstalling cachetools-4.2.4:\n",
            "      Successfully uninstalled cachetools-4.2.4\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.0.1\n",
            "    Uninstalling pymongo-4.0.1:\n",
            "      Successfully uninstalled pymongo-4.0.1\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 6.0.1\n",
            "    Uninstalling pyarrow-6.0.1:\n",
            "      Successfully uninstalled pyarrow-6.0.1\n",
            "  Attempting uninstall: oauth2client\n",
            "    Found existing installation: oauth2client 4.1.3\n",
            "    Uninstalling oauth2client-4.1.3:\n",
            "      Successfully uninstalled oauth2client-4.1.3\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: google-cloud-language\n",
            "    Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc3 3.11.4 requires cachetools>=4.2.1, but you have cachetools 3.1.1 which is incompatible.\n",
            "pydrive 1.3.1 requires oauth2client>=4.0.0, but you have oauth2client 3.0.0 which is incompatible.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 apache-beam-2.24.0 async-timeout-4.0.2 asynctest-0.13.0 avro-python3-1.9.2.1 cachetools-3.1.1 dill-0.3.1.1 fastavro-0.23.6 fasteners-0.17.3 frozenlist-1.3.0 fsspec-0.8.4 future-0.18.2 gcsfs-0.7.1 google-apitools-0.5.31 google-cloud-bigtable-1.7.0 google-cloud-core-1.7.2 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.7.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 hdfs-2.6.0 mock-2.0.0 multidict-6.0.2 numpy-1.20.0 oauth2client-3.0.0 pbr-5.8.1 pyarrow-0.17.1 pymongo-3.12.3 requests-2.27.1 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-3.7.4.3 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cachetools",
                  "google",
                  "numpy",
                  "pyarrow",
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0DDu27S3CJv"
      },
      "source": [
        "##**Entreglable (0.5 puntos)**\n",
        "\n",
        "Desarrollar un pipeline de preprocesamiento utilizando Apache Beam para generar datos de train, eval y test para los datos proporcionados anteriormente. Requisitos:\n",
        "\n",
        "- Proporcionar dos modos de ejecución: `train` y `test`\n",
        "- Soportar ejecuciones en local con `DirectRunner` y ejecuciones en Dataflow usando `DataFlowRunner`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1blctlxs_dPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777b8a52-8c2c-4991-91b1-104eff8d63d3"
      },
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "# Rellena con tu solución\n",
        "#\n",
        "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
        "# contributor license agreements.  See the NOTICE file distributed with\n",
        "# this work for additional information regarding copyright ownership.\n",
        "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
        "# (the \"License\"); you may not use this file except in compliance with\n",
        "# the License.  You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "\n",
        "\"\"\"A word-counting workflow.\"\"\"\n",
        "\n",
        "# pytype: skip-file\n",
        "\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import re\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import json\n",
        "\n",
        "from past.builtins import unicode\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.coders.coders import Coder\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions, DirectOptions\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# CLEANING\n",
        "STOP_WORDS = stopwords.words(\"english\")\n",
        "STEMMER = SnowballStemmer(\"english\")\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "\n",
        "class ExtractColumnsDoFn(beam.DoFn):\n",
        "    def process(self, element):\n",
        "        #se carga el archivo json\n",
        "        comentarios = json.loads(element)\n",
        "        #Se seleccionan las columnas que contienen el comentario y la etiqueta 1 o 0 (si es troll o no es troll)\n",
        "        texto_comentario = comentarios['content']\n",
        "        troll_o_no = comentarios['annotation']['label'][0]\n",
        "        int_troll_o_no = int(troll_o_no)\n",
        "        yield texto_comentario, int_troll_o_no\n",
        "\n",
        "\n",
        "class PreprocessColumnsTrainFn(beam.DoFn):\n",
        "    def process_sentiment(self, sentiment):\n",
        "        sentiment = int(sentiment)\n",
        "        if sentiment == 1:\n",
        "            return \"POSITIVE_TROLL\"\n",
        "        else:\n",
        "            return \"NEGATIVE_TROLL\"\n",
        "\n",
        "    def process_text(self, text):\n",
        "        # Remove link,user and special characters\n",
        "        stem = False\n",
        "        text = re.sub(TEXT_CLEANING_RE, \" \", str(text).lower()).strip()\n",
        "        tokens = []\n",
        "        for token in text.split():\n",
        "            if token not in STOP_WORDS:\n",
        "                if stem:\n",
        "                    tokens.append(STEMMER.stem(token))\n",
        "                else:\n",
        "                    tokens.append(token)\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def process(self, element):\n",
        "        processed_text = self.process_text(element[0])\n",
        "        processed_sentiment = self.process_sentiment(element[1])\n",
        "        yield f\"{processed_text}, {processed_sentiment}\"\n",
        "\n",
        "\n",
        "class CustomCoder(Coder):\n",
        "    \"\"\"A custom coder used for reading and writing strings\"\"\"\n",
        "\n",
        "    def __init__(self, encoding: str):\n",
        "        # latin-1\n",
        "        # iso-8859-1\n",
        "        self.enconding = encoding\n",
        "\n",
        "    def encode(self, value):\n",
        "        return value.encode(self.enconding)\n",
        "\n",
        "    def decode(self, value):\n",
        "        return value.decode(self.enconding)\n",
        "\n",
        "    def is_deterministic(self):\n",
        "        return True\n",
        "\n",
        "\n",
        "def run(argv=None, save_main_session=True):\n",
        "\n",
        "    \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\", dest=\"work_dir\", required=True, help=\"Working directory\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--input\", dest=\"input\", required=True, help=\"Input dataset in work dir\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output\",\n",
        "        dest=\"output\",\n",
        "        required=True,\n",
        "        help=\"Output path to store transformed data in work dir\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--mode\",\n",
        "        dest=\"mode\",\n",
        "        required=True,\n",
        "        choices=[\"train\", \"test\"],\n",
        "        help=\"Type of output to store transformed data\",\n",
        "    )\n",
        "\n",
        "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
        "\n",
        "    # We use the save_main_session option because one or more DoFn's in this\n",
        "    # workflow rely on global context (e.g., a module imported at module level).\n",
        "    pipeline_options = PipelineOptions(pipeline_args)\n",
        "    pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
        "    pipeline_options.view_as(DirectOptions).direct_num_workers = 0\n",
        "\n",
        "    # The pipeline will be run on exiting the with block.\n",
        "    with beam.Pipeline(options=pipeline_options) as p:\n",
        "\n",
        "        # Read the text file[pattern] into a PCollection.\n",
        "        raw_data = p | \"ReadTwitchData\" >> ReadFromText(\n",
        "            known_args.input, coder=CustomCoder(\"latin-1\")\n",
        "        )\n",
        "\n",
        "        if known_args.mode == \"train\":\n",
        "\n",
        "            transformed_data = (\n",
        "                raw_data\n",
        "                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn())\n",
        "                | \"Preprocess\" >> beam.ParDo(PreprocessColumnsTrainFn())\n",
        "            )\n",
        "\n",
        "            eval_percent = 20\n",
        "            assert 0 < eval_percent < 100, \"eval_percent must in the range (0-100)\"\n",
        "            train_dataset, eval_dataset = (\n",
        "                transformed_data\n",
        "                | \"Split dataset\"\n",
        "                >> beam.Partition(\n",
        "                    lambda elem, _: int(random.uniform(0, 100) < eval_percent), 2\n",
        "                )\n",
        "            )\n",
        "\n",
        "            train_dataset | \"TrainWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"train\", \"part\")\n",
        "            )\n",
        "            eval_dataset | \"EvalWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"eval\", \"part\")\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            transformed_data = (\n",
        "                raw_data\n",
        "                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn())\n",
        "                | \"Preprocess\" >> beam.Map(lambda x: f'\"{x[0]}\"')\n",
        "            )\n",
        "\n",
        "            transformed_data | \"TestWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"test\", \"part\")\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    run()\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing preprocess.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qawuKC0k4B0e"
      },
      "source": [
        "Se proporciona un fichero `setup.py` necesario para ejecutar en DataFlow. Modificar la variable `REQUIRED_PACKAGES` con las dependencias que se hayan usado en el `requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1MQvWsk_mVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa37aac-17ad-42fd-8e26-6e7f8d0e89f8"
      },
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "  \"apache-beam[gcp]==2.24.0\",\n",
        "  \"tensorflow==2.8.0\",\n",
        "  \"gensim==3.6.0\",\n",
        "  \"fsspec==0.8.4\",\n",
        "  \"gcsfs==0.7.1\",\n",
        "  \"numpy==1.20.0\",\n",
        "  \"keras==2.8.0\",\n",
        "]\n",
        "\n",
        "setuptools.setup(\n",
        "    name=\"twitchstreaming\",\n",
        "    version=\"0.0.1\",\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=setuptools.find_packages(),\n",
        "    include_package_data=True,\n",
        "    description=\"Troll detection\",\n",
        ")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLDlz_fL4UJA"
      },
      "source": [
        "### Validación preprocess train en local (0.25 puntos)\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFpirg37C3bN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6a1eb0b-1eb4-499b-ad0c-8e1baede066f"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/data.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f32971693b0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f32971694d0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f3297169560> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f32971695f0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f3297169680> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f32971697a0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f3297169830> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f32971698c0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f3297169950> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f3297169b90> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f3297169b00> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f3297169c20> ====================\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f3296c35450> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f3296c35a10> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/DoOnce/Impulse_16)+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_17))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)_19))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/InitializeWrite_20))+(ref_PCollection_PCollection_10/Write))+(ref_PCollection_PCollection_11/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/DoOnce/Impulse_32)+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_33))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)_35))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/InitializeWrite_36))+(ref_PCollection_PCollection_21/Write))+(ref_PCollection_PCollection_22/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_ReadTwitchData/Read/_SDFBoundedSourceWrapper/Impulse_5)+(ReadTwitchData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(ReadTwitchData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_1_split/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((((((((ref_PCollection_PCollection_1_split/Read)+(ReadTwitchData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_ExtractColumns_7))+(ref_AppliedPTransform_Preprocess_8))+(ref_AppliedPTransform_Split dataset/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)_11))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)_37))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)_21))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/WriteBundles_22))+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/Pair_23))+(TrainWriteToCSV/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/WriteBundles_38))+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/Pair_39))+(EvalWriteToCSV/Write/WriteImpl/GroupByKey/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((EvalWriteToCSV/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/Extract_41))+(ref_PCollection_PCollection_27/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_21/Read)+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/PreFinalize_42))+(ref_PCollection_PCollection_28/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((TrainWriteToCSV/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/Extract_25))+(ref_PCollection_PCollection_16/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_10/Read)+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/PreFinalize_26))+(ref_PCollection_PCollection_17/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_10/Read)+(ref_AppliedPTransform_TrainWriteToCSV/Write/WriteImpl/FinalizeWrite_27)\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.10 seconds.\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_21/Read)+(ref_AppliedPTransform_EvalWriteToCSV/Write/WriteImpl/FinalizeWrite_43)\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.10 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkkG271a4qnA"
      },
      "source": [
        "### Validación preprocess test en local (0.25 puntos)\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4lkLgecLS3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff860560-e0e4-4b7f-93bd-47685ff93551"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/data.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f4cfc68fcb0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f4cfc68fdd0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f4cfc68fe60> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f4cfc68fef0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f4cfc68ff80> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f4cfc6900e0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f4cfc690170> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f4cfc690200> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f4cfc690290> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f4cfc6904d0> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f4cfc690440> ====================\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f4cfc690560> ====================\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f4cfc3cee50> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f4cfc3ce350> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/DoOnce/Impulse_13)+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_14))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/DoOnce/Map(decode)_16))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/InitializeWrite_17))+(ref_PCollection_PCollection_7/Write))+(ref_PCollection_PCollection_8/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_ReadTwitchData/Read/_SDFBoundedSourceWrapper/Impulse_5)+(ReadTwitchData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(ReadTwitchData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_1_split/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((((ref_PCollection_PCollection_1_split/Read)+(ReadTwitchData/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_ExtractColumns_7))+(ref_AppliedPTransform_Preprocess_8))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/WindowInto(WindowIntoFn)_18))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/WriteBundles_19))+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/Pair_20))+(TestWriteToCSV/Write/WriteImpl/GroupByKey/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((TestWriteToCSV/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/Extract_22))+(ref_PCollection_PCollection_13/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_7/Read)+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/PreFinalize_23))+(ref_PCollection_PCollection_14/Write)\n",
            "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_7/Read)+(ref_AppliedPTransform_TestWriteToCSV/Write/WriteImpl/FinalizeWrite_24)\n",
            "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 2 (skipped: 0), batches: 2, num_threads: 2\n",
            "INFO:apache_beam.io.filebasedsink:Renamed 2 shards in 0.11 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geZM9Sbj45LK"
      },
      "source": [
        "## Entregable 2 (1.25 puntos)\n",
        "\n",
        "Desarrollar una tarea de entrenamiento para los datos preprocesados. Requisitos:\n",
        "\n",
        "- Soportar ejecuciones en local usando el SDK de AI-Platform y ejecuciones en GCP con el mismo código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMUwXgm_5el-"
      },
      "source": [
        "Se crea el directorio donde se dejará este entregable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMi8dI1gLoIc"
      },
      "source": [
        "%mkdir /content/batch/trainer\n",
        "%mkdir /content/batch/trainer/data"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dJyMXTuNPwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a06707f-9c26-45d0-ad3d-6308566a6a06"
      },
      "source": [
        "%%writefile trainer/__init__.py\n",
        "\n",
        "version = \"0.1.0\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing trainer/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUu6gKaTL_S2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26e4b008-7506-4c45-ba31-8500a0bdd4b1"
      },
      "source": [
        "%%writefile trainer/task.py\n",
        "\n",
        "# Rellena con tu solución\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import argparse\n",
        "import multiprocessing as mp\n",
        "import logging\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "import pickle\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    LSTM,\n",
        ")\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# WORD2VEC\n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 7\n",
        "# 32\n",
        "W2V_EPOCH = 5\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "# SENTIMENT\n",
        "#POSITIVE = \"POSITIVE\"\n",
        "#NEGATIVE = \"NEGATIVE\"\n",
        "#NEUTRAL = \"NEUTRAL\"\n",
        "#SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "WORD2VEC_MODEL = \"model.w2v\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "ENCODER_MODEL = \"encoder.pkl\"\n",
        "\n",
        "\n",
        "def generate_word2vec(train_df):\n",
        "    documents = [_text.split() for _text in train_df.text.values]\n",
        "    w2v_model = gensim.models.word2vec.Word2Vec(\n",
        "        size=W2V_SIZE,\n",
        "        window=W2V_WINDOW,\n",
        "        min_count=W2V_MIN_COUNT,\n",
        "        workers=mp.cpu_count(),\n",
        "    )\n",
        "    w2v_model.build_vocab(documents)\n",
        "\n",
        "    words = w2v_model.wv.vocab.keys()\n",
        "    vocab_size = len(words)\n",
        "    logging.info(f\"Vocab size: {vocab_size}\")\n",
        "    w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n",
        "\n",
        "    return w2v_model\n",
        "\n",
        "\n",
        "def generate_tokenizer(train_df):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_df.text)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    logging.info(f\"Total words: {vocab_size}\")\n",
        "    return tokenizer, vocab_size\n",
        "\n",
        "\n",
        "def generate_label_encoder(train_df):\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(train_df.sentiment.tolist())\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def generate_embedding(word2vec_model, vocab_size, tokenizer):\n",
        "    embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in word2vec_model.wv:\n",
        "            embedding_matrix[i] = word2vec_model.wv[word]\n",
        "    return Embedding(\n",
        "        vocab_size,\n",
        "        W2V_SIZE,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=SEQUENCE_LENGTH,\n",
        "        trainable=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def train_and_evaluate(\n",
        "    work_dir, train_df, eval_df, batch_size=1024, epochs=8, steps=1000\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Trains and evaluates the estimator given.\n",
        "    The input functions are generated by the preprocessing function.\n",
        "    \"\"\"\n",
        "\n",
        "    model_dir = os.path.join(work_dir, \"model\")\n",
        "    if tf.io.gfile.exists(model_dir):\n",
        "        tf.io.gfile.rmtree(model_dir)\n",
        "    tf.io.gfile.mkdir(model_dir)\n",
        "\n",
        "    # Specify where to store our model\n",
        "    run_config = tf.estimator.RunConfig()\n",
        "    run_config = run_config.replace(model_dir=model_dir)\n",
        "\n",
        "    # This will give us a more granular visualization of the training\n",
        "    run_config = run_config.replace(save_summary_steps=10)\n",
        "\n",
        "    # Create Word2vec of training data\n",
        "    logging.info(\"---- Generating word2vec model ----\")\n",
        "    word2vec_model = generate_word2vec(train_df)\n",
        "\n",
        "    # Tokenize training data\n",
        "    logging.info(\"---- Generating tokenizer ----\")\n",
        "    tokenizer, vocab_size = generate_tokenizer(train_df)\n",
        "\n",
        "    logging.info(\"---- Tokenizing train data ----\")\n",
        "    x_train = pad_sequences(\n",
        "        tokenizer.texts_to_sequences(train_df.text), maxlen=SEQUENCE_LENGTH\n",
        "    )\n",
        "    logging.info(\"---- Tokenizing eval data ----\")\n",
        "    x_eval = pad_sequences(\n",
        "        tokenizer.texts_to_sequences(eval_df.text), maxlen=SEQUENCE_LENGTH\n",
        "    )\n",
        "\n",
        "    # Label Encoder\n",
        "    logging.info(\"---- Generating label encoder ----\")\n",
        "    label_encoder = generate_label_encoder(train_df)\n",
        "\n",
        "    logging.info(\"---- Encoding train target ----\")\n",
        "    y_train = label_encoder.transform(train_df.sentiment.tolist())\n",
        "    logging.info(\"---- Encoding eval target ----\")\n",
        "    y_eval = label_encoder.transform(eval_df.sentiment.tolist())\n",
        "\n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_eval = y_eval.reshape(-1, 1)\n",
        "\n",
        "    # Create Embedding Layer\n",
        "    logging.info(\"---- Generating embedding layer ----\")\n",
        "    embedding_layer = generate_embedding(word2vec_model, vocab_size, tokenizer)\n",
        "\n",
        "    logging.info(\"---- Generating Sequential model ----\")\n",
        "    model = Sequential()\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    logging.info(\"---- Adding loss function to model ----\")\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    logging.info(\"---- Adding callbacks to model ----\")\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", patience=5, cooldown=0),\n",
        "        EarlyStopping(monitor=\"val_accuracy\", min_delta=1e-4, patience=5),\n",
        "    ]\n",
        "\n",
        "    logging.info(\"---- Training model ----\")\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        steps_per_epoch=steps,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.1,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    logging.info(\"---- Evaluating model ----\")\n",
        "    score = model.evaluate(x_eval, y_eval, batch_size=batch_size)\n",
        "    logging.info(f\"ACCURACY: {score[1]}\")\n",
        "    logging.info(f\"LOSS: {score[0]}\")\n",
        "\n",
        "    logging.info(\"---- Saving models ----\")\n",
        "    pickle.dump(\n",
        "        tokenizer,\n",
        "        tf.io.gfile.GFile(os.path.join(model_dir, TOKENIZER_MODEL), mode=\"wb\"),\n",
        "        protocol=0,\n",
        "    )\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n",
        "        with tf.io.gfile.GFile(\n",
        "            os.path.join(model_dir, KERAS_MODEL), mode=\"wb\"\n",
        "        ) as gcs_file:\n",
        "            model.save(local_file.name)\n",
        "            gcs_file.write(local_file.read())\n",
        "\n",
        "    # word2vec_model.save(os.path.join(model_dir, WORD2VEC_MODEL))\n",
        "\n",
        "    # pickle.dump(\n",
        "    #     label_encoder, open(os.path.join(model_dir, ENCODER_MODEL), \"wb\"), protocol=0\n",
        "    # )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    \"\"\"Main function called by AI Platform.\"\"\"\n",
        "\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--job-dir\",\n",
        "        help=\"Directory for staging trainer files. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\",\n",
        "        required=True,\n",
        "        help=\"Directory for staging and working files. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=1024,\n",
        "        help=\"Batch size for training and evaluation.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--epochs\", type=int, default=8, help=\"Number of epochs to train the model\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--steps\",\n",
        "        type=int,\n",
        "        default=1000,\n",
        "        help=\"Number of steps per epoch to train the model\",\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_data_files = tf.io.gfile.glob(\n",
        "        os.path.join(args.work_dir, \"transformed_data/train/part-*\")\n",
        "    )\n",
        "    eval_data_files = tf.io.gfile.glob(\n",
        "        os.path.join(args.work_dir, \"transformed_data/eval/part-*\")\n",
        "    )\n",
        "\n",
        "    train_df = pd.concat(\n",
        "        [\n",
        "            pd.read_csv(\n",
        "                f,\n",
        "                names=[\"text\", \"sentiment\"],\n",
        "                dtype={\"text\": \"string\", \"sentiment\": \"string\"},\n",
        "            )\n",
        "            for f in train_data_files\n",
        "        ]\n",
        "    ).dropna()\n",
        "\n",
        "    eval_df = pd.concat(\n",
        "        [\n",
        "            pd.read_csv(\n",
        "                f,\n",
        "                names=[\"text\", \"sentiment\"],\n",
        "                dtype={\"text\": \"string\", \"sentiment\": \"string\"},\n",
        "            )\n",
        "            for f in eval_data_files\n",
        "        ]\n",
        "    ).dropna()\n",
        "\n",
        "    train_and_evaluate(\n",
        "        args.work_dir,\n",
        "        train_df=train_df,\n",
        "        eval_df=eval_df,\n",
        "        batch_size=args.batch_size,\n",
        "        epochs=args.epochs,\n",
        "        steps=args.steps,\n",
        "    )\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing trainer/task.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8axPHdHX6d9W"
      },
      "source": [
        "### Validación Train en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos preprocesados del apartado anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9997ZzsLmq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c694011d-a20f-4b2a-a769-9712a75a154f"
      },
      "source": [
        "# Explicitly tell `gcloud ai-platform local train` to use Python 3 \n",
        "! gcloud config set ml_engine/local_python $(which python3)\n",
        "\n",
        "# This is similar to `python -m trainer.task --job-dir local-training-output`\n",
        "# but it better replicates the AI Platform environment, especially for\n",
        "# distributed training (not applicable here).\n",
        "! gcloud ai-platform local train \\\n",
        "  --package-path trainer \\\n",
        "  --module-name trainer.task \\\n",
        "  -- \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --epochs 1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [ml_engine/local_python].\n",
            "INFO:tensorflow:TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--work-dir', '/content/batch', '--epochs', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}\n",
            "INFO:root:---- Generating word2vec model ----\n",
            "INFO:gensim.models.word2vec:collecting all words and their counts\n",
            "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 79257 words, keeping 10914 word types\n",
            "INFO:gensim.models.word2vec:collected 14886 word types from a corpus of 114949 raw words and 15986 sentences\n",
            "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
            "INFO:gensim.models.word2vec:effective_min_count=10 retains 1603 unique words (10% of original 14886, drops 13283)\n",
            "INFO:gensim.models.word2vec:effective_min_count=10 leaves 84956 word corpus (73% of original 114949, drops 29993)\n",
            "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 14886 items\n",
            "INFO:gensim.models.word2vec:sample=0.001 downsamples 56 most-common words\n",
            "INFO:gensim.models.word2vec:downsampling leaves estimated 70668 word corpus (83.2% of prior 84956)\n",
            "INFO:gensim.models.base_any2vec:estimated required memory for 1603 words and 300 dimensions: 4648700 bytes\n",
            "INFO:gensim.models.word2vec:resetting layer weights\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nhlkcjn62Zo"
      },
      "source": [
        "## Entregable 3 (0.5 puntos)\n",
        "\n",
        "Desarrollar un pipeline de inferencia utilizando Apache Beam para generar predicciones usando los modelos generados en el apartado anterior así como los de test generados en el primer entregable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHBpLXB-OmjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1652e68-137d-409e-adff-adaaff1a0b1c"
      },
      "source": [
        "%%writefile predict.py\n",
        "\n",
        "# Rellena con tu solución\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import tempfile\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "from apache_beam.coders.coders import Coder\n",
        "\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE_TROLL = \"POSITIVE_TROLL\"\n",
        "NEGATIVE_TROLL = \"NEGATIVE_TROLL\"\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "\n",
        "\n",
        "class Predict(beam.DoFn):\n",
        "    def __init__(\n",
        "        self, model_dir,\n",
        "    ):\n",
        "        self.model_dir = model_dir\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def setup(self):\n",
        "        keras_model_path = os.path.join(self.model_dir, KERAS_MODEL)\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n",
        "            with tf.io.gfile.GFile(keras_model_path, mode=\"rb\") as gcs_file:\n",
        "                local_file.write(gcs_file.read())\n",
        "                self.model = tf.keras.models.load_model(local_file.name)\n",
        "\n",
        "        tokenizer_path = os.path.join(self.model_dir, TOKENIZER_MODEL)\n",
        "        self.tokenizer = pickle.load(tf.io.gfile.GFile(tokenizer_path, mode=\"rb\"))\n",
        "\n",
        "    def decode_sentiment(self, score):\n",
        "            return NEGATIVE_TROLL if score < 0.5 else POSITIVE_TROLL\n",
        "\n",
        "    def process(self, element):\n",
        "        start_at = time.time()\n",
        "        # Tokenize text\n",
        "        x_test = pad_sequences(\n",
        "            self.tokenizer.texts_to_sequences([element]), maxlen=SEQUENCE_LENGTH\n",
        "        )\n",
        "        # Predict\n",
        "        score = self.model.predict([x_test])[0]\n",
        "        # Decode sentiment\n",
        "        label = self.decode_sentiment(score)\n",
        "\n",
        "        yield {\n",
        "            \"text\": element,\n",
        "            \"label\": label,\n",
        "            \"score\": float(score),\n",
        "            \"elapsed_time\": time.time() - start_at,\n",
        "        }\n",
        "\n",
        "\n",
        "class CustomCoder(Coder):\n",
        "    \"\"\"A custom coder used for reading and writing strings\"\"\"\n",
        "\n",
        "    def __init__(self, encoding: str):\n",
        "        # latin-1\n",
        "        # iso-8859-1\n",
        "        self.enconding = encoding\n",
        "\n",
        "    def encode(self, value):\n",
        "        return value.encode(self.enconding)\n",
        "\n",
        "    def decode(self, value):\n",
        "        return value.decode(self.enconding)\n",
        "\n",
        "    def is_deterministic(self):\n",
        "        return True\n",
        "\n",
        "\n",
        "def run(model_dir, source, sink, beam_options=None):\n",
        "    with beam.Pipeline(options=beam_options) as p:\n",
        "        _ = (\n",
        "            p\n",
        "            | \"Read data\" >> source\n",
        "            # | \"Preprocess\" >> beam.ParDo(PreprocessTextFn(model_dir, \"ID\"))\n",
        "            | \"Predict\" >> beam.ParDo(Predict(model_dir))\n",
        "            | \"Format as JSON\" >> beam.Map(json.dumps)\n",
        "            | \"Write predictions\" >> sink\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"Main function\"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\",\n",
        "        dest=\"work_dir\",\n",
        "        required=True,\n",
        "        help=\"Directory for temporary files and preprocessed datasets to. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--model-dir\",\n",
        "        dest=\"model_dir\",\n",
        "        required=True,\n",
        "        help=\"Path to the exported TensorFlow model. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    verbs = parser.add_subparsers(dest=\"verb\")\n",
        "    batch_verb = verbs.add_parser(\"batch\", help=\"Batch prediction\")\n",
        "    batch_verb.add_argument(\n",
        "        \"--inputs-dir\",\n",
        "        dest=\"inputs_dir\",\n",
        "        required=True,\n",
        "        help=\"Input directory where CSV data files are read from. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "    batch_verb.add_argument(\n",
        "        \"--outputs-dir\",\n",
        "        dest=\"outputs_dir\",\n",
        "        required=True,\n",
        "        help=\"Directory to store prediction results. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    args, pipeline_args = parser.parse_known_args()\n",
        "    print(args)\n",
        "    beam_options = PipelineOptions(pipeline_args)\n",
        "    beam_options.view_as(SetupOptions).save_main_session = True\n",
        "    # beam_options.view_as(DirectOptions).direct_num_workers = 0\n",
        "\n",
        "    project = beam_options.view_as(GoogleCloudOptions).project\n",
        "\n",
        "    if args.verb == \"batch\":\n",
        "        results_prefix = os.path.join(args.outputs_dir, \"part\")\n",
        "\n",
        "        source = ReadFromText(args.inputs_dir, coder=CustomCoder(\"latin-1\"))\n",
        "        sink = WriteToText(results_prefix)\n",
        "\n",
        "    else:\n",
        "        parser.print_usage()\n",
        "        sys.exit(1)\n",
        "\n",
        "    run(args.model_dir, source, sink, beam_options)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUdgcLIP7a42"
      },
      "source": [
        "Generamos un timestamp para la ejecución de las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pvKMtuQPOlr"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkLMdMup70ne"
      },
      "source": [
        "### Validación Predict en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta inferencia usando los modelos anteriores y los datos de test generados anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUJN0XCLPR_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9dfafa-4a3c-4b2d-8516-15498c674d31"
      },
      "source": [
        "! python3 predict.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --model-dir $WORK_DIR/model \\\n",
        "  batch \\\n",
        "  --inputs-dir $WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $WORK_DIR/predictions/$TIMESTAMP"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(inputs_dir='/content/batch/transformed_data/test/part-00000-of-00002', model_dir='/content/batch/model', outputs_dir='/content/batch/predictions/2022-02-26_20-27-26', verb='batch', work_dir='/content/batch')\n",
            "2022-02-26 20:30:31.686502: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um68jx_F8mjF"
      },
      "source": [
        "##Entregable 4 (1.25 puntos)\n",
        "\n",
        "En este entregable se validará el funcionamiento del código en un proyecto de GCP sobre DataFlow y AI Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JHC9dCT83FH"
      },
      "source": [
        "Establecemos el bucket y region de GCP sobre el que trabajaremos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgUpX__KQvt-"
      },
      "source": [
        "GCP_WORK_DIR = 'gs://practica-desp-alg-sergio-buck'\n",
        "GCP_REGION = 'europe-west1'\n",
        "#GCP_REGION = 'us-central1'"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A66WAfm9DmU"
      },
      "source": [
        "### Validación preprocess train en Dataflow (0.25 puntos)\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en GCP con el servicio DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJYVuWyJP1Ya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "143df635-ac49-467c-9e0a-2fe057113a6e"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
            "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
            "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
            "INFO:oauth2client.client:Refreshing access_token\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmprp8kxej2']\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
            "\n",
            "warning: check: missing required meta-data: url\n",
            "\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
            "\n",
            "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmprp8kxej2', 'apache-beam==2.24.0', '--no-deps', '--no-binary', ':all:']\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 490, in run\n",
            "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 951, in communicate\n",
            "    stdout = self.stdout.read()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"preprocess.py\", line 193, in <module>\n",
            "    run()\n",
            "  File \"preprocess.py\", line 187, in run\n",
            "    os.path.join(known_args.output, \"test\", \"part\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 555, in __exit__\n",
            "    self.result = self.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 521, in run\n",
            "    allow_proto_holders=True).run(False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 534, in run\n",
            "    return self.runner.run_pipeline(self, self._options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py\", line 479, in run_pipeline\n",
            "    artifacts=environments.python_sdk_dependencies(options)))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/transforms/environments.py\", line 613, in python_sdk_dependencies\n",
            "    staged_name in stager.Stager.create_job_resources(options, tmp_dir))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 235, in create_job_resources\n",
            "    resources.extend(Stager._create_beam_sdk(sdk_remote_location, temp_dir))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 625, in _create_beam_sdk\n",
            "    sdk_local_file = Stager._download_pypi_sdk_package(temp_dir)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 731, in _download_pypi_sdk_package\n",
            "    processes.check_output(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/utils/processes.py\", line 91, in check_output\n",
            "    out = subprocess.check_output(*args, **kwargs)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 411, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 512, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 866, in __exit__\n",
            "    self._wait(timeout=self._sigint_wait_secs)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1647, in _wait\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8czKRMb99ZRX"
      },
      "source": [
        "### Validación preprocess test en Dataflow (0.25 puntos)\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en GCP con el servicio DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45mfSRGdQLBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f5ad7c-1a80-4ca1-b543-0a622e064543"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
            "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
            "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
            "INFO:oauth2client.client:Refreshing access_token\n",
            "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmpraqek5y7']\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 490, in run\n",
            "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 951, in communicate\n",
            "    stdout = self.stdout.read()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"preprocess.py\", line 193, in <module>\n",
            "    run()\n",
            "  File \"preprocess.py\", line 187, in run\n",
            "    os.path.join(known_args.output, \"test\", \"part\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 555, in __exit__\n",
            "    self.result = self.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 521, in run\n",
            "    allow_proto_holders=True).run(False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 534, in run\n",
            "    return self.runner.run_pipeline(self, self._options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py\", line 479, in run_pipeline\n",
            "    artifacts=environments.python_sdk_dependencies(options)))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/transforms/environments.py\", line 613, in python_sdk_dependencies\n",
            "    staged_name in stager.Stager.create_job_resources(options, tmp_dir))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 190, in create_job_resources\n",
            "    setup_options.setup_file, temp_dir, build_setup_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 578, in _build_setup_package\n",
            "    processes.check_output(build_setup_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/utils/processes.py\", line 91, in check_output\n",
            "    out = subprocess.check_output(*args, **kwargs)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 411, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 512, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 849, in __exit__\n",
            "    self.stdout.close()\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFNxYtxW9fP9"
      },
      "source": [
        "### Validación Train en AI Platform (0.5 puntos)\n",
        "\n",
        "Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos de las ejecuciones anteriores en GCP con los datos obtenidos almacenados en Google Cloud Storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0fP_5Gy9zfz"
      },
      "source": [
        "Generamos un nombre para el job de entrenamiento y donde se almacenarán los metadatos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eU0FkyLQPt6"
      },
      "source": [
        "JOB = \"troll_detection_batch_$(date +%Y%m%d_%H%M%S)\"\n",
        "JOB_DIR = GCP_WORK_DIR + \"/trainer\""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z33nVs5QSEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290b9617-fea5-4949-e388-13a163779cb9"
      },
      "source": [
        "! gcloud ai-platform jobs submit training $JOB \\\n",
        "  --module-name trainer.task \\\n",
        "  --package-path trainer \\\n",
        "  --scale-tier basic_gpu \\\n",
        "  --python-version 3.7 \\\n",
        "  --runtime-version 2.1 \\\n",
        "  --region $GCP_REGION \\\n",
        "  --job-dir $JOB_DIR \\\n",
        "  --stream-logs \\\n",
        "  -- \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --epochs 1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.jobs.submit.training) HttpError accessing <https://ml.googleapis.com/v1/projects/practica-desp-alg-sergio/jobs?alt=json>: response: <{'vary': 'Origin, X-Origin, Referer', 'content-type': 'application/json; charset=UTF-8', 'content-encoding': 'gzip', 'date': 'Sat, 26 Feb 2022 23:13:50 GMT', 'server': 'ESF', 'cache-control': 'private', 'x-xss-protection': '0', 'x-frame-options': 'SAMEORIGIN', 'x-content-type-options': 'nosniff', 'transfer-encoding': 'chunked', 'status': 429}>, content <{\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"Quota failure for project practica-desp-alg-sergio. The request for 1 K80 accelerators exceeds the allowed maximum of 0 A100, 0 K80, 0 P100, 0 P4, 0 T4, 0 TPU_V2, 0 TPU_V2_POD, 0 TPU_V3, 0 TPU_V3_POD, 0 V100 accelerators. To read more about Cloud ML Engine quota, see https://cloud.google.com/ml-engine/quotas.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"subject\": \"practica-desp-alg-sergio\",\n",
            "            \"description\": \"The request for 1 K80 accelerators exceeds the allowed maximum of 0 A100, 0 K80, 0 P100, 0 P4, 0 T4, 0 TPU_V2, 0 TPU_V2_POD, 0 TPU_V3, 0 TPU_V3_POD, 0 V100 accelerators.\"\n",
            "          }\n",
            "        ]\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            ">\n",
            "This may be due to network connectivity issues. Please check your network settings, and the status of the service you are trying to reach.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu5SOWsy9886"
      },
      "source": [
        "### Validación predict en Dataflow (0.25 puntos)\n",
        "\n",
        "Con el comando mostrado a continuación se valida la predicción correcta de los datos de test usando los modelos generados en el comando anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXoHCgMg-LUD"
      },
      "source": [
        "Generamos un timestamp para el almacenamiento de las inferencias en Google Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmcetQtnQjVo"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT_NxsKDQlxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba91578-eaaf-477f-87c4-01a4564d2121"
      },
      "source": [
        "# For using sample models: --model-dir gs://$BUCKET_NAME/models/\n",
        "! python3 predict.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --model-dir $GCP_WORK_DIR/model/ \\\n",
        "  batch \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --inputs-dir $GCP_WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $GCP_WORK_DIR/predictions/$TIMESTAMP"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(inputs_dir='gs://practica-desp-alg-sergio-buck/transformed_data/test/part*', model_dir='gs://practica-desp-alg-sergio-buck/model/', outputs_dir='gs://practica-desp-alg-sergio-buck/predictions/2022-02-26_20-31-58', verb='batch', work_dir='gs://practica-desp-alg-sergio-buck')\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
            "\n",
            "warning: check: missing required meta-data: url\n",
            "\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 490, in run\n",
            "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 951, in communicate\n",
            "    stdout = self.stdout.read()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"predict.py\", line 165, in <module>\n",
            "    run(args.model_dir, source, sink, beam_options)\n",
            "  File \"predict.py\", line 104, in run\n",
            "    | \"Write predictions\" >> sink\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 555, in __exit__\n",
            "    self.result = self.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 521, in run\n",
            "    allow_proto_holders=True).run(False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 534, in run\n",
            "    return self.runner.run_pipeline(self, self._options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py\", line 479, in run_pipeline\n",
            "    artifacts=environments.python_sdk_dependencies(options)))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/transforms/environments.py\", line 613, in python_sdk_dependencies\n",
            "    staged_name in stager.Stager.create_job_resources(options, tmp_dir))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 235, in create_job_resources\n",
            "    resources.extend(Stager._create_beam_sdk(sdk_remote_location, temp_dir))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 625, in _create_beam_sdk\n",
            "    sdk_local_file = Stager._download_pypi_sdk_package(temp_dir)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/stager.py\", line 731, in _download_pypi_sdk_package\n",
            "    processes.check_output(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/utils/processes.py\", line 91, in check_output\n",
            "    out = subprocess.check_output(*args, **kwargs)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 411, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 512, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 866, in __exit__\n",
            "    self._wait(timeout=self._sigint_wait_secs)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1647, in _wait\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GWbfFvwQTGS"
      },
      "source": [
        "# Inferencia online\n",
        "\n",
        "En esta segunda parte de la práctica se realizará un microservicio de inferencia online usando los modelos generados en la primera parte. Para esta parte de la práctica el código de vuestro microservicio deberá estar subido en un repositorio. En la variable de debajo deberéis dejar la URL a vuestro repositorrio pues será el contenido con el que serás evaluado. \n",
        "\n",
        "**Importante:** asegúrate de crear el repositorio de manera pública para poder clonarlo.\n",
        "\n",
        "A continuación os dejo un diagrama con la arquitectura que se va a desarrollar:\n",
        "\n",
        "![online_diagram](https://drive.google.com/uc?export=view&id=1zR7Cwp0Vq1QeTxwLoJ8YJNRM9G5KVh2S)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REn23OCIBjXF"
      },
      "source": [
        "#REPOSITORIO = \"https://gitkc.cloud/despliegue-de-algoritmos-viii/troll-detection-online-service.git\"\n",
        "REPOSITORIO = \"https://github.com/SergioOrenga/practica-despliegue-de-algoritmos.git\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqBT-b_8BsIR"
      },
      "source": [
        "Creamos el directorio donde trabajaremos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAskyBUAQZHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6f0ccd-ba49-461f-ddb6-0591c8ad645d"
      },
      "source": [
        "%mkdir /content/online\n",
        "%cd /content/online"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/online\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/online"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw7GiXq4Pe09",
        "outputId": "bd2ee085-cb71-4dd6-cb62-5ebe26828fec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/online\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlj7lGW3Q6CS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9becbb1c-0ca1-4cbd-c4c7-8a97368345f5"
      },
      "source": [
        "# Clone the repository\n",
        "! git clone $REPOSITORIO\n",
        "\n",
        "# Set the working directory to the sample code directory\n",
        "#%cd ./troll-detection-online-service\n",
        "%cd ./practica-despliegue-de-algoritmos\n",
        "\n",
        "# Change to develop\n",
        "! git checkout develop"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'practica-despliegue-de-algoritmos'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 59 (delta 17), reused 45 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (59/59), done.\n",
            "/content/online/practica-despliegue-de-algoritmos\n",
            "Branch 'develop' set up to track remote branch 'develop' from 'origin'.\n",
            "Switched to a new branch 'develop'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TNWU-sMLAATJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FndkRszYQ8_f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3dcc7ee5-906e-4a43-eeb3-38dcc4239cc9"
      },
      "source": [
        "! pip install -r requirements.txt"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting requests==2.25.0\n",
            "  Downloading requests-2.25.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▍                          | 10 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 20 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 40 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 51 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting uvicorn==0.12.2\n",
            "  Downloading uvicorn-0.12.2-py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting fastapi==0.61.2\n",
            "  Downloading fastapi-0.61.2-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 25.8 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.4.1)\n",
            "Collecting tensorflow==2.1.0\n",
            "  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8 MB 23 kB/s \n",
            "\u001b[?25hCollecting loguru==0.5.3\n",
            "  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (3.6.0)\n",
            "Requirement already satisfied: fsspec==0.8.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.8.4)\n",
            "Requirement already satisfied: gcsfs==0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.7.1)\n",
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.0->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.0->-r requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.12.2->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.12.2->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Collecting starlette==0.13.6\n",
            "  Downloading starlette-0.13.6-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting pydantic<2.0.0,>=1.0.0\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 29.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->-r requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->-r requirements.txt (line 5)) (3.1.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (0.37.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 67.1 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.13.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (1.43.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r requirements.txt (line 7)) (3.3.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0->-r requirements.txt (line 9)) (5.2.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 11)) (4.4.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 11)) (3.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 11)) (0.4.6)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 11)) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (57.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (3.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (4.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 11)) (0.4.8)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 7)) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 11)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 7)) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements.txt (line 7)) (3.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 11)) (3.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (21.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 11)) (1.7.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=4596fb7c08ab0bc350e1a819dc832ef3ae94b92e910e3e9bf6be0d2663e03c62\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: requests, numpy, h5py, tensorflow-estimator, tensorboard, starlette, pydantic, keras-applications, h11, gast, uvicorn, tensorflow, scikit-learn, loguru, fastapi\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.20.0\n",
            "    Uninstalling numpy-1.20.0:\n",
            "      Successfully uninstalled numpy-1.20.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "pymc3 3.11.4 requires cachetools>=4.2.1, but you have cachetools 3.1.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.25.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed fastapi-0.61.2 gast-0.2.2 h11-0.13.0 h5py-2.10.0 keras-applications-1.0.8 loguru-0.5.3 numpy-1.19.5 pydantic-1.9.0 requests-2.25.0 scikit-learn-0.23.2 starlette-0.13.6 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 uvicorn-0.12.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IefrYp6tJ4PE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5046004a-3c28-48df-cb27-5273c43494c7"
      },
      "source": [
        "! pip install pyngrok"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=357fe55d8a041db3b092f9457162f6910f18a96728014818fdb56a184cc927d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNde3TEzCmjg"
      },
      "source": [
        "Para cuando estes modificando, probando y ejecutando ficheros os dejo en las celdas de abajo los comandos de git necesarios para interaccionar con vuestro repositorio en caso de que queráis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsjWKcYZC6mm"
      },
      "source": [
        "! git status"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuVHFtqvC8Kw"
      },
      "source": [
        "! git add <files>\n",
        "! git commit -m \"Nuevos cambios\"\n",
        "! git push origin master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H88vcTU_DDpv"
      },
      "source": [
        "Será necesario definir y establecer la variable de entorno `DEFAULT_MODEL_PATH` para definir donde están almacenados nuestros modelos para hacer inferencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn9xsgx4u5xP"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"DEFAULT_MODEL_PATH\"] = \"/content/batch/model/\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-IslHWwDSTU"
      },
      "source": [
        "### Validación inferencia online en local (1.75 puntos)\n",
        "\n",
        "Se validará la correcta inferencia del microservio en local utilizando Swagger. Para ejecutar en local solo hay que ejecutar los comandos a continuación. Después, entrar en la URL proporcionada por ngrock `<ngrok_url>/docs` para acceder a swagger y probar la inferencia como vimos en clase."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ngrok authtoken 25dBgbdsihEHHotBo7D5Kffr4Ok_4vrXXCHNaEo6Lg5cCQCXa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyR2ZSZaJMkp",
        "outputId": "d505aef3-428a-42db-ac91-2a3a1de46420"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81vGToBWJ_h-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c64e60b-62e4-4282-8bfc-436cc357cd5b"
      },
      "source": [
        "# For testing purposes\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: http://f9c3-35-185-242-87.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/online/practica-despliegue-de-algoritmos/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FvU1FhJQyge",
        "outputId": "50770c22-3cc4-49f5-c118-8a4f322d1c79"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/online/practica-despliegue-de-algoritmos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFeSh7rcuG7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23541cd4-c000-4757-d0f7-bcf590197340"
      },
      "source": [
        "! uvicorn app.main:app --port 8000"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-02-26 20:38:53.704673: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2022-02-26 20:38:53.704819: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2022-02-26 20:38:53.704841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m2638\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32m2022-02-26 20:38:55.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.core.event_handlers\u001b[0m:\u001b[36mstartup\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mRunning app start handler.\u001b[0m\n",
            "2022-02-26 20:38:55.301419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-02-26 20:38:55.314539: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2022-02-26 20:38:55.314602: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fb836fd336aa): /proc/driver/nvidia/version does not exist\n",
            "2022-02-26 20:38:55.315021: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-02-26 20:38:55.322813: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2022-02-26 20:38:55.323675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bd5b060840 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-02-26 20:38:55.323715: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     37.134.124.23:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     37.134.124.23:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     37.134.124.23:0 - \"\u001b[1mGET /docs HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     37.134.124.23:0 - \"\u001b[1mGET /openapi.json HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     37.134.124.23:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     37.134.124.23:0 - \"\u001b[1mGET /testing HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     37.134.124.23:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     37.134.124.23:0 - \"\u001b[1mGET /docs HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     37.134.124.23:0 - \"\u001b[1mGET /openapi.json HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32m2022-02-26 20:55:04.479\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mapp.services.models\u001b[0m:\u001b[36m_pre_process\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mPre-processing payload.\u001b[0m\n",
            "\u001b[32m2022-02-26 20:55:04.480\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mapp.services.models\u001b[0m:\u001b[36m_predict\u001b[0m:\u001b[36m47\u001b[0m - \u001b[34m\u001b[1mPredicting.\u001b[0m\n",
            "\u001b[32m2022-02-26 20:55:04.841\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mapp.services.models\u001b[0m:\u001b[36m_post_process\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mPost-processing prediction.\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     37.134.124.23:0 - \"\u001b[1mPOST /api/model/predict HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     37.134.124.23:0 - \"\u001b[1mGET /api/model/predict HTTP/1.1\u001b[0m\" \u001b[31m405 Method Not Allowed\u001b[0m\n",
            "\u001b[32m2022-02-26 20:57:31.659\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mapp.services.models\u001b[0m:\u001b[36m_pre_process\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mPre-processing payload.\u001b[0m\n",
            "\u001b[32m2022-02-26 20:57:31.659\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mapp.services.models\u001b[0m:\u001b[36m_predict\u001b[0m:\u001b[36m47\u001b[0m - \u001b[34m\u001b[1mPredicting.\u001b[0m\n",
            "\u001b[32m2022-02-26 20:57:31.765\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mapp.services.models\u001b[0m:\u001b[36m_post_process\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mPost-processing prediction.\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     37.134.124.23:0 - \"\u001b[1mPOST /api/model/predict HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m2638\u001b[0m]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuLtOjukEMIl"
      },
      "source": [
        "### Validación inferencia online en GCP (1.75 puntos)\n",
        "\n",
        "Se validará el correcto funcionamiento del microservicio haciendo una petición POST de inferencia a través de curl al microservicio desplegado en GCP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX1MiTGuD2KS"
      },
      "source": [
        "Primero, contruiremos una imagen Docker con el microservicio y subiremos el desarrollo al Container Repository en GCP a través de Cloud Build."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Los siguientes pasos es obligatorio realizarlos para seguir con la práctica.**\n",
        "\n",
        "1.   Selecciona o crea un proyecto en GCP\n",
        "2.   Asegurate de que la facturación está activada para tu proyecto.\n",
        "3.   [Habilita la API de Google Cloud Storage](https://console.cloud.google.com/apis/library/storage-component.googleapis.com?q=storage).\n",
        "4. [Habilita la API de Google Cloud Registry](https://console.cloud.google.com/apis/library/containerregistry.googleapis.com?q=container).\n",
        "5. [Habilita la API de Google Cloud Run](https://console.cloud.google.com/apis/library/run.googleapis.com?q=cloud%20run).\n",
        "6. [Habilita la API de Google Cloud Build](https://console.cloud.google.com/apis/library/cloudbuild.googleapis.com?q=cloud%20build).\n",
        "7. [Habilita la API de App Engine Flexible Environment](https://console.cloud.google.com/apis/library/appengineflex.googleapis.com?q=app%20eng).\n",
        "8. [Habilita la API de App Engine Admin](https://console.cloud.google.com/apis/library/appengine.googleapis.com?q=app%20engine).\n",
        "9. Introduce tu ID de proyecto de GCP en la celda de abajo. Ejecuta la celda para asegurarnos de que el Cloud SDK usa el proyecto adecuado para todos los comandos en este notebook.\n",
        "\n",
        "**Nota**: Jupyter ejecuta las lineas con el prefijo `!` como comandos shell de consola, y puede usar variables de Python en los comandos añadiendoles el prefijo `$`."
      ],
      "metadata": {
        "id": "MZnp4y451-Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/online/practica-despliegue-de-algoritmos\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKONYzEWMKhc",
        "outputId": "f00d2f0a-63e1-4e3b-b575-f2cee143cac8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/online/practica-despliegue-de-algoritmos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"DEFAULT_MODEL_PATH\"] = \"/content/online/practica-despliegue-de-algoritmos/\""
      ],
      "metadata": {
        "id": "2IwKepDYBZ-C"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil -m cp \\\n",
        "  \"gs://$BUCKET_NAME/model/model.h5\" \\\n",
        "  \"gs://$BUCKET_NAME/model/tokenizer.pkl\" \\\n",
        "  ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCL5Ri3OBmBF",
        "outputId": "c543b5db-51fd-435f-8b9b-2d58336c4a64"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://practica-desp-alg-sergio-buck/model/model.h5...\n",
            "Copying gs://practica-desp-alg-sergio-buck/model/tokenizer.pkl...\n",
            "\\ [2/2 files][ 19.7 MiB/ 19.7 MiB] 100% Done                                    \n",
            "Operation completed over 2 objects/19.7 MiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRhq-d_E7-CY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2879d468-ee69-4eda-e8fc-98ecaa7c554a"
      },
      "source": [
        "! gcloud builds submit --tag gcr.io/$PROJECT_ID/troll-detection-online-service"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating temporary tarball archive of 41 file(s) totalling 19.7 MiB before compression.\n",
            "Uploading tarball of [.] to [gs://practica-desp-alg-sergio_cloudbuild/source/1645913263.503983-57294f7a63e4426092dcd5f90eb39401.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/practica-desp-alg-sergio/locations/global/builds/5d3e2967-ddd5-4da8-9912-8a489a0034f8].\n",
            "Logs are available at [https://console.cloud.google.com/cloud-build/builds/5d3e2967-ddd5-4da8-9912-8a489a0034f8?project=974781368807].\n",
            " REMOTE BUILD OUTPUT\n",
            "starting build \"5d3e2967-ddd5-4da8-9912-8a489a0034f8\"\n",
            "\n",
            "FETCHSOURCE\n",
            "Fetching storage object: gs://practica-desp-alg-sergio_cloudbuild/source/1645913263.503983-57294f7a63e4426092dcd5f90eb39401.tgz#1645913264884059\n",
            "Copying gs://practica-desp-alg-sergio_cloudbuild/source/1645913263.503983-57294f7a63e4426092dcd5f90eb39401.tgz#1645913264884059...\n",
            "/ [1 files][  3.8 MiB/  3.8 MiB]                                                \n",
            "Operation completed over 1 objects/3.8 MiB.\n",
            "BUILD\n",
            "Already have image (with digest): gcr.io/cloud-builders/docker\n",
            "Sending build context to Docker daemon  20.71MB\n",
            "Step 1/4 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
            "python3.7: Pulling from tiangolo/uvicorn-gunicorn-fastapi\n",
            "df5590a8898b: Pulling fs layer\n",
            "705bb4cb554e: Pulling fs layer\n",
            "519df5fceacd: Pulling fs layer\n",
            "ccc287cbeddc: Pulling fs layer\n",
            "e3f8e6af58ed: Pulling fs layer\n",
            "aebed27b2d86: Pulling fs layer\n",
            "3b81dff756ff: Pulling fs layer\n",
            "6e4f59a23b58: Pulling fs layer\n",
            "5cdb61eb416d: Pulling fs layer\n",
            "415755650c07: Pulling fs layer\n",
            "82af7648a68a: Pulling fs layer\n",
            "697e3cdf6fdb: Pulling fs layer\n",
            "8d414b25a011: Pulling fs layer\n",
            "3e3cfa737f56: Pulling fs layer\n",
            "1595db7eb9b1: Pulling fs layer\n",
            "d2d3fcd4b870: Pulling fs layer\n",
            "35f87fd78dc8: Pulling fs layer\n",
            "146e266752fc: Pulling fs layer\n",
            "57df244389c0: Pulling fs layer\n",
            "e8391aa94779: Pulling fs layer\n",
            "ccc287cbeddc: Waiting\n",
            "e3f8e6af58ed: Waiting\n",
            "aebed27b2d86: Waiting\n",
            "3b81dff756ff: Waiting\n",
            "6e4f59a23b58: Waiting\n",
            "5cdb61eb416d: Waiting\n",
            "415755650c07: Waiting\n",
            "82af7648a68a: Waiting\n",
            "697e3cdf6fdb: Waiting\n",
            "8d414b25a011: Waiting\n",
            "3e3cfa737f56: Waiting\n",
            "1595db7eb9b1: Waiting\n",
            "d2d3fcd4b870: Waiting\n",
            "35f87fd78dc8: Waiting\n",
            "146e266752fc: Waiting\n",
            "57df244389c0: Waiting\n",
            "e8391aa94779: Waiting\n",
            "705bb4cb554e: Verifying Checksum\n",
            "705bb4cb554e: Download complete\n",
            "519df5fceacd: Verifying Checksum\n",
            "519df5fceacd: Download complete\n",
            "df5590a8898b: Verifying Checksum\n",
            "df5590a8898b: Download complete\n",
            "ccc287cbeddc: Verifying Checksum\n",
            "ccc287cbeddc: Download complete\n",
            "aebed27b2d86: Verifying Checksum\n",
            "aebed27b2d86: Download complete\n",
            "6e4f59a23b58: Verifying Checksum\n",
            "6e4f59a23b58: Download complete\n",
            "3b81dff756ff: Verifying Checksum\n",
            "3b81dff756ff: Download complete\n",
            "415755650c07: Verifying Checksum\n",
            "415755650c07: Download complete\n",
            "5cdb61eb416d: Verifying Checksum\n",
            "5cdb61eb416d: Download complete\n",
            "697e3cdf6fdb: Download complete\n",
            "82af7648a68a: Verifying Checksum\n",
            "82af7648a68a: Download complete\n",
            "8d414b25a011: Verifying Checksum\n",
            "8d414b25a011: Download complete\n",
            "3e3cfa737f56: Verifying Checksum\n",
            "3e3cfa737f56: Download complete\n",
            "1595db7eb9b1: Verifying Checksum\n",
            "1595db7eb9b1: Download complete\n",
            "d2d3fcd4b870: Verifying Checksum\n",
            "d2d3fcd4b870: Download complete\n",
            "35f87fd78dc8: Verifying Checksum\n",
            "35f87fd78dc8: Download complete\n",
            "e3f8e6af58ed: Verifying Checksum\n",
            "e3f8e6af58ed: Download complete\n",
            "146e266752fc: Download complete\n",
            "e8391aa94779: Verifying Checksum\n",
            "e8391aa94779: Download complete\n",
            "57df244389c0: Verifying Checksum\n",
            "57df244389c0: Download complete\n",
            "df5590a8898b: Pull complete\n",
            "705bb4cb554e: Pull complete\n",
            "519df5fceacd: Pull complete\n",
            "ccc287cbeddc: Pull complete\n",
            "e3f8e6af58ed: Pull complete\n",
            "aebed27b2d86: Pull complete\n",
            "3b81dff756ff: Pull complete\n",
            "6e4f59a23b58: Pull complete\n",
            "5cdb61eb416d: Pull complete\n",
            "415755650c07: Pull complete\n",
            "82af7648a68a: Pull complete\n",
            "697e3cdf6fdb: Pull complete\n",
            "8d414b25a011: Pull complete\n",
            "3e3cfa737f56: Pull complete\n",
            "1595db7eb9b1: Pull complete\n",
            "d2d3fcd4b870: Pull complete\n",
            "35f87fd78dc8: Pull complete\n",
            "146e266752fc: Pull complete\n",
            "57df244389c0: Pull complete\n",
            "e8391aa94779: Pull complete\n",
            "Digest: sha256:f5770422a8875fe3d677e1bda724eeba332b53b0a348a9cdde854ba7136c66be\n",
            "Status: Downloaded newer image for tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
            " ---> bc3531b1f244\n",
            "Step 2/4 : COPY ./requirements.txt /requirements.txt\n",
            " ---> 172ec6c20c14\n",
            "Step 3/4 : RUN pip install -r /requirements.txt\n",
            " ---> Running in f52180a39b9e\n",
            "Collecting requests==2.25.0\n",
            "  Downloading requests-2.25.0-py2.py3-none-any.whl (61 kB)\n",
            "Collecting uvicorn==0.12.2\n",
            "  Downloading uvicorn-0.12.2-py3-none-any.whl (45 kB)\n",
            "Collecting fastapi==0.61.2\n",
            "  Downloading fastapi-0.61.2-py3-none-any.whl (48 kB)\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "Collecting scikit-learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
            "Collecting tensorflow==2.1.0\n",
            "  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
            "Collecting loguru==0.5.3\n",
            "  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n",
            "Collecting gensim==3.6.0\n",
            "  Downloading gensim-3.6.0.tar.gz (23.1 MB)\n",
            "Collecting fsspec==0.8.4\n",
            "  Downloading fsspec-0.8.4-py3-none-any.whl (91 kB)\n",
            "Collecting gcsfs==0.7.1\n",
            "  Downloading gcsfs-0.7.1-py2.py3-none-any.whl (20 kB)\n",
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "Collecting chardet<4,>=3.0.2\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "Collecting idna<3,>=2.5\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.7/site-packages (from uvicorn==0.12.2->-r /requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from uvicorn==0.12.2->-r /requirements.txt (line 2)) (3.10.0.2)\n",
            "Collecting click==7.*\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "Collecting starlette==0.13.6\n",
            "  Downloading starlette-0.13.6-py3-none-any.whl (59 kB)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/site-packages (from fastapi==0.61.2->-r /requirements.txt (line 3)) (1.8.2)\n",
            "Collecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting joblib>=0.11\n",
            "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
            "Collecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.13.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (79 kB)\n",
            "Collecting astor>=0.6.0\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.1.0->-r /requirements.txt (line 7)) (0.37.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "Collecting keras-preprocessing>=1.1.0\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Collecting google-pasta>=0.1.6\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting grpcio>=1.8.6\n",
            "  Downloading grpcio-1.44.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "Collecting absl-py>=0.7.0\n",
            "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Collecting protobuf>=3.8.0\n",
            "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
            "Collecting smart_open>=1.2.1\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "Collecting decorator\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting google-auth-oauthlib\n",
            "  Downloading google_auth_oauthlib-0.5.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting google-auth>=1.2\n",
            "  Downloading google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
            "Collecting google-auth>=1.2\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "Collecting google-auth-oauthlib\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r /requirements.txt (line 7)) (57.5.0)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r /requirements.txt (line 7)) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r /requirements.txt (line 7)) (3.6.0)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
            "Collecting charset-normalizer<3.0,>=2.0\n",
            "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting attrs>=17.3.0\n",
            "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "Building wheels for collected packages: gensim, gast, termcolor\n",
            "  Building wheel for gensim (setup.py): started\n",
            "  Building wheel for gensim (setup.py): finished with status 'done'\n",
            "  Created wheel for gensim: filename=gensim-3.6.0-cp37-cp37m-linux_x86_64.whl size=24602130 sha256=1370d249312e79b9374fdd341af11ee9b377c60ec5c430606d68c2cf392ce96d\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/c8/f9/afb722099bdb5d73e5807019ce1512fd065502ccc15ea2b5bd\n",
            "  Building wheel for gast (setup.py): started\n",
            "  Building wheel for gast (setup.py): finished with status 'done'\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=35a37ca39b6369fa2233438f71dc8882bd9dd4514056e5643ba2c7b8add4bf41\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "  Building wheel for termcolor (setup.py): started\n",
            "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=6d1d955a70120fabf75f1df11cb1a90d26281c3b524d08e8a394f6a20d8ba9a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "Successfully built gensim gast termcolor\n",
            "Installing collected packages: urllib3, pyasn1, idna, chardet, certifi, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, numpy, multidict, google-auth, frozenlist, yarl, werkzeug, protobuf, markdown, h5py, grpcio, google-auth-oauthlib, charset-normalizer, attrs, asynctest, async-timeout, aiosignal, absl-py, wrapt, threadpoolctl, termcolor, tensorflow-estimator, tensorboard, starlette, smart-open, scipy, opt-einsum, keras-preprocessing, keras-applications, joblib, google-pasta, gast, fsspec, decorator, click, astor, aiohttp, uvicorn, tensorflow, scikit-learn, loguru, gensim, gcsfs, fastapi\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.14.2\n",
            "    Uninstalling starlette-0.14.2:\n",
            "      Successfully uninstalled starlette-0.14.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.0.1\n",
            "    Uninstalling click-8.0.1:\n",
            "      Successfully uninstalled click-8.0.1\n",
            "  Attempting uninstall: uvicorn\n",
            "    Found existing installation: uvicorn 0.15.0\n",
            "    Uninstalling uvicorn-0.15.0:\n",
            "      Successfully uninstalled uvicorn-0.15.0\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.68.1\n",
            "    Uninstalling fastapi-0.68.1:\n",
            "      Successfully uninstalled fastapi-0.68.1\n",
            "Successfully installed absl-py-1.0.0 aiohttp-3.8.1 aiosignal-1.2.0 astor-0.8.1 async-timeout-4.0.2 asynctest-0.13.0 attrs-21.4.0 cachetools-4.2.4 certifi-2021.10.8 chardet-3.0.4 charset-normalizer-2.0.12 click-7.1.2 decorator-5.1.1 fastapi-0.61.2 frozenlist-1.3.0 fsspec-0.8.4 gast-0.2.2 gcsfs-0.7.1 gensim-3.6.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 h5py-2.10.0 idna-2.10 joblib-1.1.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 loguru-0.5.3 markdown-3.3.6 multidict-6.0.2 numpy-1.19.5 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.0 requests-oauthlib-1.3.1 rsa-4.8 scikit-learn-0.23.2 scipy-1.4.1 six-1.16.0 smart-open-5.2.1 starlette-0.13.6 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0 threadpoolctl-3.1.0 urllib3-1.26.8 uvicorn-0.12.2 werkzeug-2.0.3 wrapt-1.13.3 yarl-1.7.2\n",
            "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
            "\u001b[0mRemoving intermediate container f52180a39b9e\n",
            " ---> 0407963f7f0b\n",
            "Step 4/4 : COPY ./app /app/app\n",
            " ---> 9fdc089cb330\n",
            "Successfully built 9fdc089cb330\n",
            "Successfully tagged gcr.io/practica-desp-alg-sergio/troll-detection-online-service:latest\n",
            "PUSH\n",
            "Pushing gcr.io/practica-desp-alg-sergio/troll-detection-online-service\n",
            "The push refers to repository [gcr.io/practica-desp-alg-sergio/troll-detection-online-service]\n",
            "25a21b24a808: Preparing\n",
            "64c77451c7a0: Preparing\n",
            "8dbc23cdef9b: Preparing\n",
            "574be7dd3a19: Preparing\n",
            "4a24608ea27b: Preparing\n",
            "c660563b938b: Preparing\n",
            "e313cca5ff4a: Preparing\n",
            "ad6638039b02: Preparing\n",
            "4c503f7b7cec: Preparing\n",
            "49022fdb47b6: Preparing\n",
            "f4cc5099892d: Preparing\n",
            "471ef6255d6e: Preparing\n",
            "0f8cd2835d79: Preparing\n",
            "0616239ad62a: Preparing\n",
            "4eb436881a0f: Preparing\n",
            "7fe8548565b4: Preparing\n",
            "8364a493536b: Preparing\n",
            "c1792902851c: Preparing\n",
            "c272c95c3fb0: Preparing\n",
            "3054497613e6: Preparing\n",
            "d35dc7f4c79e: Preparing\n",
            "dabfe5b2ea81: Preparing\n",
            "5e6a409f30b6: Preparing\n",
            "c660563b938b: Waiting\n",
            "e313cca5ff4a: Waiting\n",
            "ad6638039b02: Waiting\n",
            "4c503f7b7cec: Waiting\n",
            "49022fdb47b6: Waiting\n",
            "f4cc5099892d: Waiting\n",
            "471ef6255d6e: Waiting\n",
            "0f8cd2835d79: Waiting\n",
            "0616239ad62a: Waiting\n",
            "4eb436881a0f: Waiting\n",
            "7fe8548565b4: Waiting\n",
            "8364a493536b: Waiting\n",
            "c1792902851c: Waiting\n",
            "c272c95c3fb0: Waiting\n",
            "3054497613e6: Waiting\n",
            "d35dc7f4c79e: Waiting\n",
            "dabfe5b2ea81: Waiting\n",
            "5e6a409f30b6: Waiting\n",
            "4a24608ea27b: Layer already exists\n",
            "574be7dd3a19: Layer already exists\n",
            "c660563b938b: Layer already exists\n",
            "e313cca5ff4a: Layer already exists\n",
            "4c503f7b7cec: Layer already exists\n",
            "ad6638039b02: Layer already exists\n",
            "f4cc5099892d: Layer already exists\n",
            "49022fdb47b6: Layer already exists\n",
            "471ef6255d6e: Layer already exists\n",
            "0f8cd2835d79: Layer already exists\n",
            "4eb436881a0f: Layer already exists\n",
            "0616239ad62a: Layer already exists\n",
            "8364a493536b: Layer already exists\n",
            "7fe8548565b4: Layer already exists\n",
            "c1792902851c: Layer already exists\n",
            "c272c95c3fb0: Layer already exists\n",
            "3054497613e6: Layer already exists\n",
            "d35dc7f4c79e: Layer already exists\n",
            "5e6a409f30b6: Layer already exists\n",
            "dabfe5b2ea81: Layer already exists\n",
            "25a21b24a808: Pushed\n",
            "8dbc23cdef9b: Pushed\n",
            "64c77451c7a0: Pushed\n",
            "latest: digest: sha256:76446c14dc05ca6a5625a1100b95faf4c605eb5ef28abc163562f3434d9a1ebc size: 5134\n",
            "DONE\n",
            "\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                  IMAGES                                                                    STATUS\n",
            "5d3e2967-ddd5-4da8-9912-8a489a0034f8  2022-02-26T22:07:45+00:00  5M18S     gs://practica-desp-alg-sergio_cloudbuild/source/1645913263.503983-57294f7a63e4426092dcd5f90eb39401.tgz  gcr.io/practica-desp-alg-sergio/troll-detection-online-service (+1 more)  SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYVptQ0WEEd-"
      },
      "source": [
        "Desplegaremos la imagen Docker generada en el Container Registry en el servicio de Cloud Run. Después, validaremos que las inferencias funcionan en GCP usando el comando mostrado a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC0honui-W23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5756e446-5a7e-46a5-d3cc-17b6d63831de"
      },
      "source": [
        "! curl -X POST \"https://troll-detection-online-service-ynlyj5vetq-ew.a.run.app//api/model/predict\" -H  \"accept: application/json\" -H  \"Content-Type: application/json\" -d \"{\\\"text\\\":\\\"i hate you\\\"}\""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"label\":\"NEGATIVE_TROLL\",\"score\":0.4037797749042511,\"elapsed_time\":0.2701411247253418}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvBWPoGGKwuJ"
      },
      "source": [
        "# Detección de Trolls en Twitch en Streaming\n",
        "\n",
        "En esta última parte de la práctica se realizará un pipeline de inferencia en tiempo real de un chat de Twitch alcualmente en vivo. Para ello, usaremos mi canal de Twitch `https://www.twitch.tv/franalgaba` donde tengo un bot deplegado poniendo mensajes troll y no troll de forma aleatoria del dataset que hemos usado en la primera parte.\n",
        "\n",
        "Para acceder al chat de Twitch os proporciono el conector correspondiente que será desplegado como Cloud Function como hicimos en clase y usando mis credenciales recogerá los mensajes del chat y los enviará a un topic de Pub/Sub en GCP. Después, desarrollarás un job en streaming de Dataflow con el que leerás esos mensajes de Pub/sub, los mandarás a tu microservicio de inferencia para que haga las predicciones y enviarás los resultados a un nuevo tópico de Pub/Sub.\n",
        "\n",
        "A continuación os dejo un diagrama con la arquitectura que se va a desarrollar:\n",
        "\n",
        "![streaming_diagram](https://drive.google.com/uc?export=view&id=1TEBPPc9ZF09IM5iGq9FwGAx9PVzAYNPg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw3oY8n5GjuU"
      },
      "source": [
        "Primero, creamos el publisher que será el encargado de recoger los mensajes de Twitch y enviarlos a Pub/Sub. Esto os lo doy yo desarrollado, sólo tendréis que desplegarlo en una Cloud Function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Los siguientes pasos es obligatorio realizarlos para seguir con la práctica.**\n",
        "\n",
        "1.   Selecciona o crea un proyecto en GCP\n",
        "2.   Asegurate de que la facturación está activada para tu proyecto.\n",
        "3.   [Habilita la API de Google Cloud Pub/Sub](https://console.cloud.google.com/apis/library/pubsub.googleapis.com?q=pub).\n",
        "4. [Habilita la API de Google Cloud Function](https://console.cloud.google.com/apis/library/cloudfunctions.googleapis.com?q=function).\n",
        "5. Introduce tu ID de proyecto de GCP en la celda de abajo. Ejecuta la celda para asegurarnos de que el Cloud SDK usa el proyecto adecuado para todos los comandos en este notebook.\n",
        "\n",
        "**Nota**: Jupyter ejecuta las lineas con el prefijo `!` como comandos shell de consola, y puede usar variables de Python en los comandos añadiendoles el prefijo `$`."
      ],
      "metadata": {
        "id": "uJ5QoWM5Yimw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH8WcRKrKwKy"
      },
      "source": [
        "%mkdir -p /content/streaming/publisher"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75whqxX7K54B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4abac89b-85ac-491b-cae7-95fec35a267f"
      },
      "source": [
        "# Execute after restart\n",
        "%cd /content/streaming/publisher"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/streaming/publisher\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyOecviAK8QO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ee12f9-1442-4ab9-c7d3-88e6969795f0"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "twitchio==1.2.3\n",
        "loguru==0.5.3\n",
        "google-cloud-pubsub==2.1.0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sluNYquPY8JH",
        "outputId": "e6c4ba18-e436-4a0e-d153-c60e7a44d911"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting twitchio==1.2.3\n",
            "  Downloading twitchio-1.2.3-py3-none-any.whl (48 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▊                         | 10 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 20 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 30 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 40 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 48 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.5.3)\n",
            "Collecting google-cloud-pubsub==2.1.0\n",
            "  Downloading google_cloud_pubsub-2.1.0-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 30 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40 kB 43.7 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 61 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 92 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 102 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 122 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 133 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 153 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 163 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 174 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 177 kB 18.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: async-timeout>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from twitchio==1.2.3->-r requirements.txt (line 2)) (4.0.2)\n",
            "Requirement already satisfied: aiohttp>=3.3 in /usr/local/lib/python3.7/dist-packages (from twitchio==1.2.3->-r requirements.txt (line 2)) (3.8.1)\n",
            "Collecting websockets>=6.0\n",
            "  Downloading websockets-10.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.26.3)\n",
            "Collecting libcst>=0.3.10\n",
            "  Downloading libcst-0.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 62.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (0.12.3)\n",
            "Collecting proto-plus>=1.7.1\n",
            "  Downloading proto_plus-1.20.3-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (21.4.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3->twitchio==1.2.3->-r requirements.txt (line 2)) (2.0.12)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (21.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (3.17.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (2.25.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.54.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.43.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (3.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (0.2.8)\n",
            "Collecting pyyaml>=5.2\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.7 MB/s \n",
            "\u001b[?25hCollecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (3.0.7)\n",
            "Collecting protobuf>=3.12.0\n",
            "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 43.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-pubsub==2.1.0->-r requirements.txt (line 4)) (2021.10.8)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Installing collected packages: protobuf, mypy-extensions, typing-inspect, pyyaml, websockets, proto-plus, libcst, twitchio, google-cloud-pubsub\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: google-cloud-pubsub\n",
            "    Found existing installation: google-cloud-pubsub 1.7.0\n",
            "    Uninstalling google-cloud-pubsub-1.7.0:\n",
            "      Successfully uninstalled google-cloud-pubsub-1.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive 1.3.1 requires oauth2client>=4.0.0, but you have oauth2client 3.0.0 which is incompatible.\u001b[0m\n",
            "Successfully installed google-cloud-pubsub-2.1.0 libcst-0.4.1 mypy-extensions-0.4.3 proto-plus-1.20.3 protobuf-3.19.4 pyyaml-6.0 twitchio-1.2.3 typing-inspect-0.7.1 websockets-10.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-pnzR-jMkZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4c02f18-4cb7-4670-b150-a0fdc179f35f"
      },
      "source": [
        "%%writefile main.py\n",
        "\n",
        "import os  # for importing env vars for the bot to use\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import asyncio\n",
        "\n",
        "from twitchio.ext import commands\n",
        "from google.cloud import pubsub_v1\n",
        "from loguru import logger\n",
        "\n",
        "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
        "TOPIC_NAME = os.getenv(\"TOPIC_NAME\")\n",
        "\n",
        "TOPIC_PATH = f\"projects/{PROJECT_ID}/topics/{TOPIC_NAME}\"\n",
        "\n",
        "publisher = pubsub_v1.PublisherClient()\n",
        "\n",
        "\n",
        "class Bot(commands.Bot):\n",
        "\n",
        "    def __init__(self, irc_token='...', client_id='...', nick='...', prefix=\"!\", initial_channels=['...'], debug=True):\n",
        "        super().__init__(irc_token=irc_token, client_id=client_id, nick=nick, prefix='!',\n",
        "                         initial_channels=initial_channels)\n",
        "        self.debug = debug\n",
        "\n",
        "    # Events don't need decorators when subclassed\n",
        "    async def event_ready(self):\n",
        "        logger.info('Ready')\n",
        "\n",
        "    async def event_message(self, message):\n",
        "        logger.info(message.content)\n",
        "        publisher.publish(TOPIC_PATH, str.encode(message.content))\n",
        "\n",
        "\n",
        "def main(request):\n",
        "\n",
        "    topic_name = f\"projects/{PROJECT_ID}/topics/{TOPIC_NAME}\"\n",
        "    # publisher.create_topic(topic_name)\n",
        "\n",
        "    request_json = request.get_json(silent=True)\n",
        "\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "\n",
        "    logger.info(\"Starting listener...\")\n",
        "    if \"debug\" in request_json and isinstance(request_json[\"debug\"], bool):\n",
        "        logger.info(f\"Debug mode: {request_json['debug']}\")\n",
        "        bot = Bot(\n",
        "          # set up the bot\n",
        "          irc_token=\"oauth:xl5cpf8qe8tl1d03dppymchi6r04iz\",\n",
        "          client_id=\"ciliqxi534iwg4pfqj7swl1jmkt23y\",\n",
        "          nick=\"franalgaba\",\n",
        "          prefix=\"!\",\n",
        "          initial_channels=[\"franalgaba\"],\n",
        "          debug=request_json['debug'])\n",
        "    else:\n",
        "        bot = Bot(\n",
        "          # set up the bot\n",
        "          irc_token=\"oauth:xl5cpf8qe8tl1d03dppymchi6r04iz\",\n",
        "          client_id=\"ciliqxi534iwg4pfqj7swl1jmkt23y\",\n",
        "          nick=\"franalgaba\",\n",
        "          prefix=\"!\",\n",
        "          initial_channels=[\"franalgaba\"])\n",
        "\n",
        "    bot.run()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDrQjJbsVO-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c067a81-bccc-4f1b-b8be-472a315cbf58"
      },
      "source": [
        "# In case user service error...\n",
        "! gcloud iam service-accounts add-iam-policy-binding <project_id>@appspot.gserviceaccount.com --member=user:<mail> --role=roles/iam.serviceAccountUser"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: project_id: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x_BX3FCG3mD"
      },
      "source": [
        "Para lanzar vuestra Cloud Function, que recoja y mande mensajes solo tenéis que ejecutar el comando siguiente (haced los pasos vistos en clase para desplegar el servicio):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcemNpnsKxFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240979dc-81fa-4482-ec3f-bf36f7ca2d55"
      },
      "source": [
        "#! curl -X POST https://us-central1-twitch-practice.cloudfunctions.net/bot-reciever -H \"Content-Type:application/json\"  -d '{\"debug\": false}'\n",
        "! curl -X POST https://europe-west1-practica-desp-alg-sergio.cloudfunctions.net/twitch-publisher -H \"Content-Type:application/json\"  -d '{\"debug\": false}'\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr3SmFrUHCvN"
      },
      "source": [
        "## Entregable 1 (3 puntos)\n",
        "\n",
        "En este entregable desarrollarás un pipeline de inferencia en streaming usando Apache Beam para ejecutar en Dataflow un job en streaming que llamará a vuestro microservicio para realizar inferencias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwlDH0KFbn1q"
      },
      "source": [
        "%mkdir /content/streaming/subscriber"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLNtrkgybtHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b944cd2-b8f0-4ad9-9d4f-8090a6181f2a"
      },
      "source": [
        "%cd /content/streaming/subscriber"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/streaming/subscriber\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtBrcF2zbz04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d18a9fff-529a-4d15-8957-429d6c1f07c4"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "apache-beam[gcp]==2.24.0\n",
        "fsspec==0.8.4\n",
        "gcsfs==0.7.1\n",
        "loguru==0.5.3"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RN9KNMob1jY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "652c9942-7482-4116-d665-3bfd2d2c640b"
      },
      "source": [
        "! pip install -r requirements.txt"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apache-beam[gcp]==2.24.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.24.0)\n",
            "Requirement already satisfied: fsspec==0.8.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.8.4)\n",
            "Requirement already satisfied: gcsfs==0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.7.1)\n",
            "Requirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (2.25.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (0.4.6)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (1.35.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.12.3)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7)\n",
            "Requirement already satisfied: pyarrow<0.18.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.17.1)\n",
            "Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.18.2)\n",
            "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.17.4)\n",
            "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: fastavro<0.24,>=0.21.4 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.23.6)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.19.4)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.43.0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.9.2.1)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.3.1.1)\n",
            "Requirement already satisfied: numpy<2,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.16.1)\n",
            "Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.5.31)\n",
            "Requirement already satisfied: cachetools<4,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: google-cloud-bigquery<2,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.21.0)\n",
            "Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.2.2)\n",
            "Requirement already satisfied: google-cloud-dlp<2,>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: google-cloud-datastore<2,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.8.0)\n",
            "Collecting google-cloud-pubsub<2,>=0.39.0\n",
            "  Using cached google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144 kB)\n",
            "Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.19.1)\n",
            "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7.0)\n",
            "Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.7.2)\n",
            "Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.7/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 4)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 4)) (4.8)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.26.3)\n",
            "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.12.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (1.54.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (21.3)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.7/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (5.8.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]==2.24.0->-r requirements.txt (line 2)) (3.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 4)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.7.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 4)) (21.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 4)) (3.2.0)\n",
            "Installing collected packages: google-cloud-pubsub\n",
            "  Attempting uninstall: google-cloud-pubsub\n",
            "    Found existing installation: google-cloud-pubsub 2.1.0\n",
            "    Uninstalling google-cloud-pubsub-2.1.0:\n",
            "      Successfully uninstalled google-cloud-pubsub-2.1.0\n",
            "Successfully installed google-cloud-pubsub-1.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMeJPR90b3AN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86cbea1-b1ce-46b3-c2c5-1f1691fade7c"
      },
      "source": [
        "%%writefile predict.py\n",
        "\n",
        "# Rellena con tu solución\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import requests\n",
        "import json\n",
        "import sys\n",
        "\n",
        "import apache_beam as beam\n",
        "import apache_beam.transforms.window as window\n",
        "from apache_beam.options.pipeline_options import (\n",
        "    GoogleCloudOptions,\n",
        "    StandardOptions,\n",
        "    PipelineOptions,\n",
        "    SetupOptions,\n",
        ")\n",
        "from loguru import logger\n",
        "\n",
        "\n",
        "class Predict(beam.DoFn):\n",
        "    def __init__(self, predict_server) -> None:\n",
        "        self.url = predict_server\n",
        "\n",
        "    def _predict(self, text) -> str:\n",
        "        payload = {\"text\": text}\n",
        "        headers = {\"accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                self.url, data=json.dumps(payload), headers=headers\n",
        "            )\n",
        "            response = json.loads(response.text)\n",
        "        except Exception:\n",
        "            response = {\"label\": \"undefined\", \"score\": 0, \"elapsed_time\": 0}\n",
        "\n",
        "        return response\n",
        "\n",
        "    def process(self, element, window=beam.DoFn.WindowParam):\n",
        "        logger.info(f\"Text to predict: {element}\")\n",
        "        result = self._predict(element)\n",
        "        result[\"text\"] = element\n",
        "        yield json.dumps(result)\n",
        "\n",
        "\n",
        "def run(predict_server, source, sink, beam_options=None):\n",
        "    with beam.Pipeline(options=beam_options) as p:\n",
        "        _ = (\n",
        "            p\n",
        "            | \"Read data from PubSub\" >> source\n",
        "            | \"decode\" >> beam.Map(lambda x: x.decode(\"utf-8\"))\n",
        "            | \"window\" >> beam.WindowInto(window.FixedWindows(15))\n",
        "            | \"Predict\" >> beam.ParDo(Predict(predict_server))\n",
        "            | \"encode\" >> beam.Map(lambda x: x.encode(\"utf-8\")).with_output_types(bytes)\n",
        "            | \"Write predictions\" >> sink\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"Main function\"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--inputs_topic\",\n",
        "        dest=\"inputs_topic\",\n",
        "        required=True,\n",
        "        help=\"Directory for temporary files and preprocessed datasets to. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--outputs_topic\",\n",
        "        dest=\"outputs_topic\",\n",
        "        required=True,\n",
        "        help=\"Directory for temporary files and preprocessed datasets to. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--predict_server\",\n",
        "        dest=\"predict_server\",\n",
        "        required=True,\n",
        "        help=\"Directory for temporary files and preprocessed datasets to. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    args, pipeline_args = parser.parse_known_args()\n",
        "    logger.info(args)\n",
        "    beam_options = PipelineOptions(pipeline_args)\n",
        "    beam_options.view_as(SetupOptions).save_main_session = True\n",
        "    # beam_options.view_as(DirectOptions).direct_num_workers = 0\n",
        "\n",
        "    project = beam_options.view_as(GoogleCloudOptions).project\n",
        "\n",
        "    if not project:\n",
        "        parser.print_usage()\n",
        "        print(\"error: argument --project is required for streaming\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    beam_options.view_as(StandardOptions).streaming = True\n",
        "\n",
        "    source = beam.io.ReadFromPubSub(\n",
        "        topic=\"projects/{}/topics/{}\".format(project, args.inputs_topic)\n",
        "    ).with_output_types(bytes)\n",
        "\n",
        "    sink = beam.io.WriteToPubSub(\n",
        "        topic=\"projects/{}/topics/{}\".format(project, args.outputs_topic)\n",
        "    )\n",
        "\n",
        "    run(args.predict_server, source, sink, beam_options)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT_Qmb16hgEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddbbe5c7-0c03-4721-b2be-ebdfa59f291f"
      },
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "    \"apache-beam[gcp]==2.24.0\",\n",
        "    \"fsspec==0.8.4\",\n",
        "    \"gcsfs==0.7.1\",\n",
        "    \"loguru==0.5.3\",\n",
        "]\n",
        "\n",
        "setuptools.setup(\n",
        "    name=\"twitchstreaming\",\n",
        "    version=\"0.0.1\",\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=setuptools.find_packages(),\n",
        "    include_package_data=True,\n",
        "    description=\"Twitch Troll Detection\",\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RdyxvdVHmnZ"
      },
      "source": [
        "### Validación inferencia en streaming\n",
        "\n",
        "Con el comando mostrado a continuación se genera un job en streaming de Dataflow. Antes de ejecutarlo, deberás crear dos topicos en Pub/Sub, `twitch-chat` donde se recibirán los mensajes de twitch, y `twitch-chat-predictions` donde se mandarán las predicciones generadas por vuestro microservicio.\n",
        "\n",
        "**Importante**: no te olvides de modificar la URL de tu microservicio de inferencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j4vzFn2wUPk"
      },
      "source": [
        "#GCP_WORK_DIR = 'gs://final-practice-test-execution'\n",
        "#GCP_REGION = 'us-east1'\n",
        "GCP_WORK_DIR = 'gs://practica-desp-alg-sergio-buck'\n",
        "GCP_REGION = 'europe-west1'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5ftKoXAhiJ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad25d77e-c4eb-48a3-dd89-14c0b38ead7d"
      },
      "source": [
        "! python3 predict.py \\\n",
        "--project $PROJECT_ID \\\n",
        "--region $GCP_REGION \\\n",
        "--runner DataflowRunner \\\n",
        "--temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "--setup_file ./setup.py \\\n",
        "--inputs_topic twitch-chat \\\n",
        "--outputs_topic twitch-chat-predictions \\\n",
        "--predict_server https://troll-detection-online-service-ynlyj5vetq-ew.a.run.app/api/model/predict \\"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-02-26 22:50:56.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mNamespace(inputs_topic='twitch-chat', outputs_topic='twitch-chat-predictions', predict_server='https://troll-detection-online-service-ynlyj5vetq-ew.a.run.app/api/model/predict')\u001b[0m\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
            "\n",
            "warning: check: missing required meta-data: url\n",
            "\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
            "\n",
            "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
            "Traceback (most recent call last):\n",
            "  File \"predict.py\", line 112, in <module>\n",
            "    run(args.predict_server, source, sink, beam_options)\n",
            "  File \"predict.py\", line 55, in run\n",
            "    | \"Write predictions\" >> sink\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/pipeline.py\", line 556, in __exit__\n",
            "    self.result.wait_until_finish()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py\", line 1633, in wait_until_finish\n",
            "    self)\n",
            "apache_beam.runners.dataflow.dataflow_runner.DataflowRuntimeException: Dataflow pipeline failed. State: FAILED, Error:\n",
            "Error processing pipeline.\n"
          ]
        }
      ]
    }
  ]
}